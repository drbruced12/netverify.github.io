<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Hongqiang Harry Liu | Netverify</title>

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Hongqiang Harry Liu | Netverify</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Hongqiang Harry Liu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Netverify is a site dedicated to expert commentary on network verification." />
<meta property="og:description" content="Netverify is a site dedicated to expert commentary on network verification." />
<link rel="canonical" href="http://localhost:4000/author-harry-liu.html" />
<meta property="og:url" content="http://localhost:4000/author-harry-liu.html" />
<meta property="og:site_name" content="Netverify" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Hongqiang Harry Liu" />
<script type="application/ld+json">
{"headline":"Hongqiang Harry Liu","url":"http://localhost:4000/author-harry-liu.html","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"}},"description":"Netverify is a site dedicated to expert commentary on network verification.","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <link rel="alternate" type="application/atom+xml" title="Netverify blog" href="/feed.xml">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YFVVT140ZR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YFVVT140ZR');
</script>


</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Netverify</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Authors</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/feed.xml" target="_blank">RSS</a>
</li>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": "Made with by Sal @wowthemesnet. "
    }, {
    "id": 2,
    "url": "http://localhost:4000/author-aaron-gember-jacobson.html",
    "title": "Aaron Gember-Jacobson",
    "body": "                        Aaron Gember-Jacobson View:         Aaron Gember-Jacobson is an Assistant Professor of Computer Science at Colgate University. Aaron's research focuses on network configuration verification and synthesis. Aaron enjoys teaching Introduction to Computing, Operating Systems, Computer Networks, and a First Year Seminar entitled 'The Unreliable Internet. ' Aaron received a Ph. D. and Master of Science in Computer Science from the University of Wisconsin-Madison and a Bachelor of Science in Computer Science from Marquette University. During his Ph. D. , Aaron was awarded the Internet Engineering Task force (IETF) Applied Networking Research Prize (2015) and an IBM Ph. D. Fellowship.                                   Posts by Aaron Gember-Jacobson:                                             		The Verification-Synthesis Spectrum	: 		  	 			In 				overview, 				research, 				network, 				verification, 				synthesis, 								By 						Aaron Gember-Jacobson, 						Aditya Akella, 								Aug 31, 2020						                        		Talk to a network operator today!	: 		  Recently, we have seen significant advances in tools that bring formal methods to networking. These tools verify if a given network satisfies important properties, automatically repai. . . 	 			In 				overview, 				research, 				network, 				verification, 				operators, 								By 						Aditya Akella, 						Aaron Gember-Jacobson, 								Jul 05, 2020						                                                          "
    }, {
    "id": 3,
    "url": "http://localhost:4000/author-aditya-akella.html",
    "title": "Aditya Akella",
    "body": "                        Aditya Akella View:         Aditya Akella is a Professor of Computer Sciences at the University of Wisconsin-Madison, where he leads the Wisconsin Internet and Systems Research (WISR) Lab, and a Visiting Scientist at Google. Aditya has also been a Visiting Researcher at Microsoft (2014), a Visiting Associate Professor at the University of Washington (2013), and a Postdoc at Stanford University (2005-2006). Aditya has received numerous awards, including a Finalist for the Blavatnik National Award for Young Scientists (2020), an H. I. Romnes Faculty Fellowship (2018), SACM Student Choice Professor of the Year (COW) Award (2017, 2019), the Internet Engineering Task Force (IETF) Applied Networking Research Prize (2015), and the ACM SIGCOMM Rising Star Award (2014). Aditya received a Ph. D. in Computer Science from Carnegie Mellon University (2005) and a B. Tech. in Computer Science from Indian Insitute of Technology, Madras (2000).                                   Posts by Aditya Akella:                                             		The Verification-Synthesis Spectrum	: 		  	 			In 				overview, 				research, 				network, 				verification, 				synthesis, 								By 						Aaron Gember-Jacobson, 						Aditya Akella, 								Aug 31, 2020						                        		Talk to a network operator today!	: 		  Recently, we have seen significant advances in tools that bring formal methods to networking. These tools verify if a given network satisfies important properties, automatically repai. . . 	 			In 				overview, 				research, 				network, 				verification, 				operators, 								By 						Aditya Akella, 						Aaron Gember-Jacobson, 								Jul 05, 2020						                                                          "
    }, {
    "id": 4,
    "url": "http://localhost:4000/author-george-varghese.html",
    "title": "George Varghese",
    "body": "                        George Varghese View:         George Varghese is a Chancellor's Professor of Computer Science at UCLA. He received his Ph. D. in 1992 from MIT after working at DEC designing DECNET protocols and products, including the bridge architecture and Gigaswitch. From 1993-1999, he was a professor at Washington University, and at UCSD from 1999 to 2013. He was the Distinguished Visitor in the computer science department at Stanford University from 2010-2011. From 2012-2016, he was a Principal Researcher and Partner at Microsoft Research working on network verification.                                   Posts by George Varghese:                                                                             		It’s the Equivalence Classes, Stupid	: 		  James Carville coined the phrase “It’s the economy, stupid” to say that what mattered in an election year was the economy. Formal methods search through vast state spaces in reasonabl. . . 	 			In 				reduction, 				symmetry, 				research, 				network, 				verification, 								By 						George Varghese, 								May 26, 2020						                                          "
    }, {
    "id": 5,
    "url": "http://localhost:4000/author-harry-liu.html",
    "title": "Hongqiang Harry Liu",
    "body": "                        Hongqiang Harry Liu View:         Hongqiang Harry Liu is a Director of Research in Alibaba Cloud and Alibaba DAMO Acedamy. He holds a PhD from the Department of Computer Science at Yale University and a bachelor's degree and a master's degree from the Department of Electronic Engineering at Tsinghua University. Before joining Alibaba, he served as a researcher at Microsoft Research Asia and was responsible for the R&amp;D and implementation of key technologies related to stable, high-performance Azure networks. His research interests cover cloud data center networks, backbone networks, mobile network transmission technologies, 5G networks, and device-side computing. He has served as a reviewer of papers in SIGCOMM and NSDI conferences and has published nearly 20 papers accepted by SIGCOMM, NSDI, and SOSP conferences. In 2014, he won the SIGCOMM Doctoral Dissertation Award - Honorable Mention.                                   Posts by Hongqiang Harry Liu:             		The Practice of Network Verification in Alibaba’s Global WAN	: 		  Alibaba has a global scale infrastructure to support its various types of online services (e. g. e-commerce, cloud computing, e-payment, etc. ), which have more than one billion users i. . . 	 			In 				research, 				industry experience, 				network verification, 								By 						Hongqiang Harry Liu, 								May 08, 2021						                                                                                                          "
    }, {
    "id": 6,
    "url": "http://localhost:4000/author-jennifer-rexford.html",
    "title": "Jennifer Rexford",
    "body": "                        {{page. title}} View:         {{ site. authors. jennifer. bio }}                                   Posts by {{page. title}}:     {% for post in site. posts %}    {% if post. authors contains  jennifer  %}    {% include main-loop-card. html %}    {% endif %}    {% endfor %}  "
    }, {
    "id": 7,
    "url": "http://localhost:4000/author-laurent-vanbever.html",
    "title": "Laurent Vanbever",
    "body": "                        {{page. title}} View:         {{ site. authors. lvanbever. bio }}                                   Posts by {{page. title}}:     {% for post in site. posts %}    {% if post. authors contains  lvanbever  %}    {% include main-loop-card. html %}    {% endif %}    {% endfor %}  "
    }, {
    "id": 8,
    "url": "http://localhost:4000/author-pamela-zave.html",
    "title": "Pamela Zave",
    "body": "                        {{page. title}} View:         {{ site. authors. pamela. bio }}                                   Posts by {{page. title}}:     {% for post in site. posts %}    {% if post. authors contains  pamela  %}    {% include main-loop-card. html %}    {% endif %}    {% endfor %}  "
    }, {
    "id": 9,
    "url": "http://localhost:4000/author-ratul-mahajan.html",
    "title": "Ratul Mahajan",
    "body": "                        {{page. title}} View:         {{ site. authors. ratul. bio }}                                   Posts by {{page. title}}:     {% for post in site. posts %}    {% if post. authors contains  ratul  %}    {% include main-loop-card. html %}    {% endif %}    {% endfor %}  "
    }, {
    "id": 10,
    "url": "http://localhost:4000/author-ryan-beckett.html",
    "title": "Ryan Beckett",
    "body": "                        {{page. title}} View:         {{ site. authors. beckett. bio }}                                   Posts by {{page. title}}:     {% for post in site. posts %}    {% if post. authors contains  beckett  %}    {% include main-loop-card. html %}    {% endif %}    {% endfor %}  "
    }, {
    "id": 11,
    "url": "http://localhost:4000/author-todd-millstein.html",
    "title": "Todd Millstein",
    "body": "                        {{page. title}} View:         {{ site. authors. todd. bio }}                                   Posts by {{page. title}}:     {% for post in site. posts %}    {% if post. authors contains  todd  %}    {% include main-loop-card. html %}    {% endif %}    {% endfor %}  "
    }, {
    "id": 12,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "{{page. title}}:     {% for author in site. authors %}                                         {{ author[1]. name }} :       (View Posts)      {{ author[1]. bio }}                         {% if author[1]. twitter %}         &nbsp;      {% endif %}       &nbsp;                                    {% endfor %}  "
    }, {
    "id": 13,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 14,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 15,
    "url": "http://localhost:4000/",
    "title": "Netverify: Network Verification and Synthesis",
    "body": "  {% if page. url ==  /  %}        {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}{% endif %}                All Articles:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}               "
    }, {
    "id": 16,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 17,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 18,
    "url": "http://localhost:4000/page2/",
    "title": "Netverify: Network Verification and Synthesis",
    "body": "  {% if page. url ==  /  %}        {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}{% endif %}                All Articles:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}               "
    }, {
    "id": 19,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 20,
    "url": "http://localhost:4000/the-practice-of-network-verification-in-alibaba-global-wan/",
    "title": "The Practice of Network Verification in Alibaba’s Global WAN",
    "body": "2021/05/08 - Alibaba has a global scale infrastructure to support its various types of online services (e. g. e-commerce, cloud computing, e-payment, etc. ), which have more than one billion users in total. This infrastructure has O(10) core data centers, O(100) PoP (point of presence) nodes, O(1000) edge sites across 23 geographical regions, and 69 availability zones over North America, Europe, Asia, and Oceania (see https://www. alibabacloud. com/global-locations).  Alibaba operates a private WAN (wide-area network) that interconnects all its data centers and peers with external ISPs around the world. The whole WAN has O(100) routers, which are from multiple vendors, and O(1000) links. Each router has O(1000) lines of configuration commands. There are also O(10, 000) IP prefixes announced over the WAN. With the fast growth of the application demands, our WAN doubles its scale every year and performs O(1000) update operations per month. In the coming 5G era, we expect the growth rate will be even higher given the massive demands on services from the edges that are closer to users. Since 2016, Alibaba starts to deploy network verification technologies inside its global WAN to fundamentally upgrade the way it enforces the reliability and efficiency of its network operations – never relying on human hands but computational logic. In this article, I want to share some of our refined experiences in deploying and using network verification techniques in the wild. Motivate: why network verification?: Alibaba’s global WAN is a typical example of an extremely complex network that has to be operated and managed correctly. Since many applications rely on the WAN, an inadvertent mistake might result in severe incidents that impact millions of users and businesses. On the other hand, since the applications and businesses are quickly expanding and growing, the WAN has to frequently perform upgrades such as adding new devices, replacing existing devices with new models, adding new capacity and routes, adding new security policies, and so forth. The rapid evolvements of the WAN bring tremendous challenges to the protection of the network reliability:    The Opaque Network: Similar to many others, Alibaba’s WAN grows organically and incrementally by years with the demands and requirements from the upper-layer applications and businesses, such that it has massive “fait accompli”. A local design or a configuration left by one operator can easily become a legacy that her successor cannot fully understand or dare not change; An old device model might have some specific behaviors by default that some operators aware of but others do not. As a result, the consequences of a seemingly straightforward operation might trigger a hidden problem of a legacy, leading to disasters.     The Human’s Nature: Human beings are typically good at creativity, intuitions, and declarative expressions but poor at mastering enormous details, dependencies, and concurrencies mechanically and repeatedly. Therefore, it fundamentally deviates from human nature if we merely relying on human operators to operate the opaque and giant network and guarantee the reliability persistently. The problem is not too evident at the beginning of the Internet era because the network scale and complexity are way more limited than today, but it becomes imperative if we want the network to keep the pace of the applications and businesses.     The Loss of Experience and Knowledge: Relying on human beings to operate and protect the network has another essential drawback which is the lack of accumulation of experiences and knowledge on the network. Despite conceptually, all experiences for operating a specific network can be passed among operators via documents, but this way is super inefficient because documents can be inaccurate or incomplete, it requires tremendous effort from a person to learn and remember all the details, and information can be lost or distorted in the transitions among people. Hence, it is far from rigorous to use human’s nature languages to keep experiences and knowledge.  One can find that all of the preceding challenges originate from the fact that we entirely rely on human operators to accomplish a mission that is impossible to them. Hence, a road towards a brighter future is to rely on software to handle the details of the network operation and let humans focus on the high-level goals of the network. For instance, if a network device needs to be replaced by a new model and all connectivity remains the same during or after the upgrade, a human operator can tell the software to do that and the latter has the entire knowledge of the network and can accurately handle all details. People call this way “Intent-Based Networking (IBN)”, which is a magnificent future of network operation and management. Network verification is an indispensable technology towards the ultimate goal of IBN, given that it provides a rigorous manner to check the correctness of network operations purely via software and mathematics. Alibaba is strongly motivated to adopt network verification as an automatic, rapid, and comprehensive way to (i) audit the current configuration snapshot for hidden errors; and (ii) check correctness and inconspicuous ambiguities of new configurations in an update. Lessons: deploying network verification is not easy: Building a pragmatic and effective configuration verification tool on a large-scale and complex WAN like ours is not easy. Network verification is a promising technology, but it requires more considerations, other complementary techniques, or even a completely new design of the existing network verification methods to make it a real product and a practical tool that can be used by network operators who have zero background on formal verification and zero tolerates when the tool performs badly. Here we summarize some lessons and actions that must be taken to make network verification practical:    Accuracy. The first reaction when our operators heard the idea of network verification is asking the question “who verifies you?”. In research, we can say that let us assume the verification tools are correct, but it is very hard to realize in practice. The users (operators) will never trust this technology again if the verifier gives an incorrect answer and triggers a severe incident. Therefore, how to make a network verifier accurate on Day 1st after the deployment is a critical but difficult task to make this technology fly. Despite that we are still exploring better ways, our solutions to leverage ground truth in the running network [2] and other tools for cross-checking [3] are effective in practice.     Usability. A user of a verifier has to tell the tool “what to verify”, while from time to time we find it is not a trivial job. While academic people create a large number of languages to allow users to express their verification intents, no one, in reality, wants to learn a new and domain-specific language just for a specific purpose. On one hand, we must deeply understand the expressions operators want to make in their daily work; while on the other hand, we need to design an intuitive, declarative but also relatively constrained interface between human and the verifier. Currently, we have two solutions. First, we make everything into Python which is a de-facto favorite script language that is masted by most of the operators; Second, we design graphics interfaces instead of programming languages. Of course, we are still exploring better ways, such as natural languages. By the way, I found the blog site has already had an article talking about the same issue from an entrepreneur and an industry researcher who both have the same opinions from their practice (https://netverify. fun/network-verification-2-0/).     Scalability. It is a fundamental problem faced by all formal methods. It is also why verification’s deployment and usage are limited in general software correctness checking. Conceptually, people apply formal verification to the network logic in hoping to avoid the scalability drawback given that the network’s logic is constrained, but in our practice, it is usually not the case especially in networks such as a global WAN. If we holistically model the whole network with a single logic format, the running time of verification can rapidly ramp up to unacceptable (tens of hours) as the target network’s size increases. We do not aware of a generic solution to the scalability issue. A high-level principle we leverage multiple times is to introduce domain knowledge to break down a large logic format into multiple smaller pieces that significantly boost the speed of computation. For instance, in Jinjing[1], we only verify the “delta” of ACL updates rather than the whole ACL configurations; In Hoyan[2], we simulate the routing propagation process and prune the brunches early to shrink the final logical formulas; In Aquila[3], we take advantage of the fact that the P4’s pipeline is a DAG such that we use a topological sort to reduce the complexity with the number of states from exponential to linear. I believe there will be more methodologies in different scenarios. It is great to see many researchers working on this topic [6, 7].  Our work in Alibaba is majorly to solve the three problems in different scenarios. We found that how to practically put technology into use can lead to even more fundamental research in the technology itself [1,2,3], besides engineering efforts. Prospect: the useability of network verification: Despite the current efforts, we found that we still have a long way to make network verification highly usable for a larger group of people. This is because the way how formal methods compute is different from how people think and work in daily life. For instance, in the beginning, operators strongly prefer network simulations or emulations (like CrystalNet[4]) just because these tools provide a command-line interface that is consistent with their and their existing scripts working environments. Most of the time, changing people’s habits and their existing toolchains is extremely hard. Therefore, how to make better usability of network verification technology is what we appeal to the whole community to pay attention to. It includes but not limited to the following aspects:    Familiar interfaces. It is critical to learn how operators’ preferences in their daily work and design a human-machine interface that is super friendly to network operators (some of whom might not be programmers). For instance, a verifier can also provide a command-line interface (showing the status of the routers) that is compatible with real routers.     Incremental verification. Most existing verifiers work on a snapshot of the entire network. However, in practice, most of the risks happen during updates. It will be a significant improvement in speed if one can perform incremental verifications – currently, one round of verification of an update can still take hours even if we have used many approaches to improve the verifier’s scalability. Also, it is desirable that the verifier can be general enough to identify the delta to verify instead of requiring the human to perform domain-specific analysis (e. g. Jinjing[1]). It is good to see people are talking about modularity in verification (https://netverify. fun/modular-verification-cna/, https://netverify. fun/toward-modular-network-verification/) and [5], which is a promising direction to solve the incremental verification problem.     Root-cause analysis. The strength of verification is the ability to catch all details and find potential errors or inconsistencies in the low-level logic. However, directly presenting the findings in the low-level details is very unfriendly to the users. Users demand that the verifier can also explain what happens in the low-level logic with very high-level and human-readable reasoning. For example, if the verifier finds an unexpected reachability issue, it should be able to trace back to the root cause in the configurations or designs, which is probably far away from the location of the reachability issue.     Intent learning from details. As mentioned above, a global WAN like Alibaba’s is also opaque in the overall intent of the whole network. The configurations in the network are accumulated over the years, and the intents of many of them have been unclear. Suppose we have an intent language to describe the network design, it is also interesting and important that we can convert the current network configurations to the declarative descriptions with the high-level intent language. Once it is done, incremental updates to the network will become much more controllable.  Conclusion: We devote ourselves to Alibaba to deploy network verification on our WAN because we believe it is a fundamentally new methodology to solve the tension between network growth and network reliability. However, it is far from straightforward to make this technique happen. Network verification is only mastered by a small group of people, so it has a long way to go to be widely accepted as a foundation in network operations. It brings both challenges and new research opportunities to us, and we hope you can jump into it and enjoy it! References: [1] Tian, Bingchuan, Xinyi Zhang, Ennan Zhai, Hongqiang Harry Liu, Qiaobo Ye, Chunsheng Wang, Xin Wu et al. “Safely and automatically updating in-network ACL configurations with intent language. ” In Proceedings of the ACM Special Interest Group on Data Communication, pp. 214-226. 2019. [2] Ye, Fangdan, Da Yu, Ennan Zhai, Hongqiang Harry Liu, Bingchuan Tian, Qiaobo Ye, Chunsheng Wang et al. “Accuracy, Scalability, Coverage: A Practical Configuration Verifier on a Global WAN. ” In Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication, pp. 599-614. 2020. [3] Tian, Bingchuan, Jiaqi Gao, Mengqi Liu, Ennan Zhai, Yanqing Chen, Yu Zhou, Li Dai, Feng Yan, Mengjing Ma, Ming Tang, Jie Lu, Xionglie Wei, Hongqiang Harry Liu, Ming Zhang, Chen Tian, Minlan Yu et al. “Aquila: A Practically Usable Verification System for production-scale Programmable Data Planes. ” In Proceedings of the ACM Special Interest Group on Data Communication, 2021. [4] Liu, Hongqiang Harry, Yibo Zhu, Jitu Padhye, Jiaxin Cao, Sri Tallapragada, Nuno P. Lopes, Andrey Rybalchenko, Guohan Lu, and Lihua Yuan. “Crystalnet: Faithfully emulating large production networks. ” In Proceedings of the 26th Symposium on Operating Systems Principles, pp. 599-613. 2017. [5] Zhang, Peng, Yuhao Huang, Aaron Gember-Jacobson, Wenbo Shi, Xu Liu, Hongkun Yang, and Zhiqiang Zuo. “Incremental Network Configuration Verification. ” In Proceedings of the 19th ACM Workshop on Hot Topics in Networks, pp. 81-87. 2020. [6] Beckett, Ryan, Aarti Gupta, Ratul Mahajan, and David Walker. “Control plane compression. ” In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, pp. 476-489. 2018. [7] Plotkin, Gordon D. , Nikolaj Bjørner, Nuno P. Lopes, Andrey Rybalchenko, and George Varghese. “Scaling network verification using symmetry and surgery. ” ACM SIGPLAN Notices 51, no. 1 (2016): 69-83. "
    }, {
    "id": 21,
    "url": "http://localhost:4000/modular-verification-cna/",
    "title": "Modular Verification in Compositional Network Architecture",
    "body": "2021/01/11 - In our first article on Compositional Network Architecture, we emphasized its value as a descriptive model. This article considers its uses as a prescriptive model of network architecture. As a prescriptive model, the goal of CNA is to allow maximum design freedom inside networks (the modules in our architecture), while ensuring that networks of all designs are composable. In keeping with this goal, the formal semantics of CNA (in Alloy) constrains intra-network mechanisms such as session protocols and routing loosely, requiring only certain data in the network state. The operators for network composition, on the other hand, are formalized completely. The semantics of a network is expressed in terms of reachability, and the semantics of a composition operator defines reachability in a composition of multiple networks. Reachability represents both end-to-end and path properties, which are frequently important for security (see below). Reachability is defined at the granularity of packet headers, so it encompasses session properties. There is a form of reachability dependent only on infrastructure (what the network “is willing” to do), and another form that includes a traffic model (what the users want the network to do). Comparing these, it is possible to verify that a network satisfies all legitimate user requirements. To realize the benefits of composability, networks should be built with external interfaces that conform recognizably to CNA. With such networks, composition can be implemented automatically and verified correct simply by checking known consistency properties at the interfaces. We now have an implementation of CNA composition in P4 [1], and are using it to explore the benefits of composability in building networks with programmable data planes. These benefits include implementation reuse and–most interesting to the readers of this site–easier verification. The Alloy model of networking (on which the semantics is based) can be used as a tool for describing classes of networks and verifying their properties. In parallel with implementation activities, experiments with Alloy modeling and verification are yielding many insights into modular verification of networks. Some of these insights are summarized below. Modular Verification of Layered Networks: From one perspective, layering is “just tunneling. ” It’s true that an overlay link is implemented by an underlay session with the mechanisms of encapsulation and decapsulation, but this reductive view misses a major truth about verification: The whole overlay network matters, because it is the whole overlay network that defines the purposes, properties, and packet traffic of its links, while the links just happen to be implemented as tunnels in some other network. To see how this works, we have investigated numerous examples in which IP networks offering enhanced services are layered on the bridged IP networks of the public Internet. The figure shows an example, a Virtual Private Network, from [1]. The enhanced services of the VPN ensure a variety of security properties. Other examples explore correctness properties for multicast, mobility, and reliability services.  In all these examples, layering enables very successful modularity in verification. In fact, in these experiments every property I looked at could be proved exclusively in one network or the other. These properties fall into two categories, in a roughly even split:    Because layering enforces a separation of concerns, some properties only need to hold in one network.     Some properties are hierarchical. It is required or assumed that they are true of the links of an overlay network. To satisfy this requirement or support this assumption, they must be true of the underlay sessions that implement these links. As a result, they are proved in the underlay network and used as axioms in the verification of the overlay network.  Verifying one layer/network at a time is much easier than verifying their composition as one monolithic network, for three major reasons:    Each network is smaller! One usually has fewer nodes than the other, and in the nodes common to both networks the forwarding rules of the monolithic version will be partitioned between the two layers. Sometimes the benefit of modularity is even greater, because the number of forwarding rules in the monolithic version is the product of the number of rules in modular layers.     Each network is more focused and specialized, and has more structure that can be exploited by verification. As a simple example, consider the VPN. An employee of the enterprise is in a coffee shop, with laptop connected to the Internet through WiFi. Its Internet address is in the coffee shop’s block, and is largely meaningless for filtering. At the same time, in the overlay VPN, it has an authenticated IP address that indicates the employee’s department, rank, and access privileges. As a result, filtering in the VPN, based on source IP address, can have pinpoint precision.     There is no need whatsoever to verify the composition of the networks, which might be buried in the details of the monolithic version. In the modular version, layering composition is performed by a verified operator with well-known properties. As an important bonus, network forwarding never alters packets, so invariant forwarding can be used as an axiom in intra-network verification. Invariant forwarding holds because a router’s usual functions of adding/deleting headers and changing header fields are all subsumed by the layering operator, and are viewed as part of composition.  A short paper on network verification [1] uses the the VPN example to explain how a number of security properties could be verified. This verification can take advantage of many existing tools for network verification, as most of the properties are familiar combinations of filtering and forwarding properties in IP networks. Verification also combines results from different proof techniques, as follows: First, mathematical proofs establish the properties of cryptographic session protocols. These session protocols implement secure overlay links. Next, path analysis in the overlay verifies that a designated path through the network consists of only secure links and trusted nodes. A Subtle Form of Layering: Subduction: In CNA, subduction is a third network-composition operator. It is a combination of bridging and layering, with an extra verification condition to make it work as expected. Although subduction may be new as a concept, it is common in networking. Roughly speaking, pure layering on the public Internet requires the participation of both session endpoints (as in the VPN example). Whenever one or both endpoints does not participate in the overlay, there is likely to be subduction. So it usually occurs whenever a special-purpose IP network is bridged with the public Internet, because only one endpoint is a user of the special-purpose network. (For the curious, “subduction” is a geological term. Find a picture, and you will see that one tectonic plate both abuts and slides under another plate, just as the public Internet is bridged with a special-purpose network, and also implements its links by means of layering. ) In our work with the P4 implementation of CNA, one case study includes some network services requiring flow affinity–all the packets of a designated flow must pass through a single stateful filter. For example, in the figure below, the blue packet in flow(A) does not require flow affinity, and it is simply routed through the network from one gateway to another. The red packet, in contrast, is in flow(B) and needs to pass through the filter filter(B). Both packets enter and leave the network at the same gateways, and we do not want the difficulty of altering normal routing to include location-dependent filtering. To keep the enforcement of flow affinity independent of normal routing, we implement it as an overlay with subduction. In this figure, the solid black lines are physical links, while the dashed black line is a virtual overlay link.  The figure below shows the architecture of this solution, superimposed on the physical path of a packet in flow(B). It enters the overlay through a bridging link from gateway to filter. In the overlay, it is routed to the filter responsible for flow(B). As it travels on a virtual link through the overlay, it is also traveling through the physical links of the underlay, because the overlay is layered on it. The packet exits the overlay through a bridging link from filter(B) to router2.  There is a fair amount of detail to be managed for the bridging and layering to work right, but this detail is all supplied automatically by our composition operators. There is also an extra constraint because of the subtlety of subduction–the two links marked with asterisks are exactly the same physical link, connecting gateway and filter machines. The two “filters” are members of different networks on the same machine, again implemented automatically by our P4 mechanisms. Finally we can get to the topic of verification! Working with models of a flow-affinity-preserving network, a general network, and subduction, I have found a set of constraints on the general network that guarantee correctness. Here correctness consists of two properties:    The overlay and general network, composed by subduction, enforce flow affinity on all selected flows.     If the overlay has no other added functions (e. g. , it does not filter packets), end-to-end reachability in the composed networks is exactly the same as in the original general network.  In practice the overlay will have added functions and will alter reachability, but the flow-affinity mechanism itself should be neutral. The result is a theorem that if a general network satisfies the constraints, then its composition with the overlay has the specified properties. These constraints are not easy find. Yet once we know what they are, they make sense and are satisfied by normal networks. Then, with only these known constraints to check, verified session affinity can be added painlessly to most networks. Correctness of a flow-affinity overlay is not something to be verified on a real network–it is far too complex, and even if it were accomplished, the effort that went into the proof could not be reused. As this example shows, the sensible approach is to prove the theorem with an abstract model, then verify that the real network satisfies the straightforward assumptions in the proof. Network Properties: In summary, the common theme of these verification experiments is that they reveal a much richer set of network properties than usually seen in the literature on network verification. This is because the structure and modularity of CNA make it possible to get closer to the purposes and specializations of each network, without having them obscured by the overall complexity of a set of composed networks. Access to this kind of design detail provides understanding, not only of the many properties that make designs work, but also of the practical constraints that make the properties hold. [1] Pamela Zave, Jennifer Rexford, John Sonchack, “The remaining improbable: Toward verifiable network services,” https://arxiv. org/abs/2009. 12861, 2020. "
    }, {
    "id": 22,
    "url": "http://localhost:4000/compositional-network-architecture/",
    "title": "Compositional Network Architecture",
    "body": "2021/01/04 - Compositional Network Architecture is a new descriptive model of networking, possibly the first since the “classic” Internet architecture of the late 1980s. You all know the classic architecture: it has physical, link, network, transport, and application layers. This article will talk about two reasons why CNA is worth your attention:  It provides precise and comprehensive descriptions of how the Internet works today. In doing so, it solves a mystery of Internet evolution.  It helps us identify recurring patterns in networking. These are problems that occur in many different, but predictable, contexts, along with a range of solutions to them. We are so excited about this way of teaching networking that we are writing a textbook based on it. A third reason, modular verification, is the most relevant to this site, so it will be covered in a follow-up article. Describing today’s Internet: CNA is different from the “classic” model of Internet architecture in several important ways, but in this article we’ll talk about just one: the definition of layering. Layering in the classic model is layering in the ordinary software-engineering sense: the overlay depends on the underlay, while the underlay is independent of the overlay and incorporates no knowledge of it. There is a fixed number of layers, each with a different function to perform. Just as each layer can be completely different from the others, the interface between two adjacent layers can be different from other interfaces. CNA, on the other hand, is based on the idea that the module of networking is a network. Each network is a microcosm of networking, with all the basic mechanisms including a namespace, nodes, links, routing, forwarding, session protocols, and directories (although some parts can be vestigial in some networks). In this model the Internet is a flexible composition of many networks; these networks differ from each other in purpose, geographical span, membership scope, and level of abstraction. Networks are composed with two operators, bridging (with the obvious meaning) and layering. In any network, a session is an instance of one of the services provided by that network. At a minimum, it is a set of packets that network users group together. According to the CNA definition of layering, one network is layered on another network if a link in the overlay network is implemented by a session in the underlay network. This means that the overlay link is virtual, while the links in the underlay network can be virtual or physical. As a consequence, layers in the new model are bigger than layers in the classic Internet architecture–for example, the classic network and transport layers of the Internet together make one CNA layer composed of IP networks bridged together. The advantage is that a bigger module is more complete, so a network’s interface to an overlay network is the same as the interface of an underlay network to it. In other words, we have made networks composable like Lego bricks.  The picture shows the headers of a typical packet in the AT&amp;T backbone network. The packet is traveling through 6 layered networks simultaneously, 3 of which are IP networks. Each IP network is revealed by an IP header for forwarding and one or more session-protocol headers. At other observation points, packets would likely have similar complexity, but with different layers, because there is no need for global agreement on compositional architecture. As explained in detail in our Communications of the ACM paper [1], this and many other observations solve a mystery of Internet evolution: how the Internet has evolved since 1993 to meet many difficult new challenges, despite the fact that there has been no major change in the IP protocol since then. Patterns in networking: Because CNA defines many ways in which all networks and network interfaces are similar, it also points to problems that most networks or compositions of networks will have to solve. The word is “most” rather than “all” because the restricted purposes of some networks, or the extra similarities between some composed networks, can simplify some problems out of existence. Viewed with enough abstraction, each predictable problem has a suite of common solutions. The problem and the solutions make a recurring pattern. Teaching these patterns seems much more meaningful and memorable than teaching detailed instances of the patterns without linking them together. We are so excited about this way of teaching networking that we are writing a textbook based on it. On the way, we have written two survey articles based on CNA, which will become chapters of the textbook. Mobility is the network function that maintains network connectivity to a machine, despite the fact that the machine is moving its attachment to the network. “The design space of network mobility” [2] explains that there are exactly two patterns for providing mobility.  In one pattern, the named machine simply moves from one access link of the network to another, and routing is updated dynamically so that the machine can still be reached under the same name. For example, MAC learning in an Ethernet can find a machine by its MAC address after it has moved.  In the other pattern, layering is involved. The mobile machine retains its name and links in the overlay, despite the fact that it is moving. In the underlay network that is implementing these links, the mobile machine has a location-dependent name, which necessarily changes as it moves. The session protocol implementing an overlay link signals the change from one session endpoint to the other, so that the stable endpoint starts sending session packets to the new name. Most proposals for adding mobility to IP networks use this pattern. These patterns had never been recognized before our paper, and it was CNA that enabled us to see them. Our other survey article takes on the subject of network security [3]. It illustrates an extremely important point, which is that CNA-based teaching of networking offers a solution to an otherwise insoluble problem, which is the explosion of topics to be covered in a networking course. The paper frames the subject of network security, presents a practical classification of security attacks, and divides all defense mechanisms into four patterns. We have planned a tutorial on this material, covering the patterns in 3 hours of lecturing. The same lectures would work in a networking course, especially supplemented by reading the article. The students might not get all the details, but they would have an easy-to-remember framework in which to fit any new security attack or defense they encounter. [1] Pamela Zave and Jennifer Rexford, “The compositional architecture of the Internet,” Communications of the ACM 62(3):78-87, March 2019 (http://www. pamelazave. com/cnaCACM19. pdf). [2] Pamela Zave and Jennifer Rexford, “The design space of network mobility,” Olivier Bonaventure and Hamed Haddadi, editors, Recent Advances in Networking (http://www. sigcomm. org/content/ebook/SIGCOMMeBook2013v1_chapter8. pdf), ACM SIGCOMM, 2013. [3] Pamela Zave and Jennifer Rexford, “Patterns and interactions in network security,” ACM Computing Surveys 53(6):118, 2020, to appear. A version with more explanation, examples, and references is available at https://arxiv. org/abs/1912. 13371. "
    }, {
    "id": 23,
    "url": "http://localhost:4000/network-verification-2-0/",
    "title": "Network verification 2.0",
    "body": "2020/12/15 - The first generation of network verification research, or Network Verification 1. 0, has been tremendously successful. It has produced a wide range of methods that enable strong guarantees about important aspects of large, complex networks. Unlike common tools such as ping and traceroute, which provide visibility into the experience of individual flows, verification tools can check the behavior for all possible flows even if that set has billions of flows. Today, most large cloud providers are using this technology to ensure high availability and security. However, outside of the large cloud providers, the vast majority of networks are still being built and operated without automated reasoning and with all of the availability and security risks that go with such an operation. We argue that the goal of the next generation of network verification research, or Network Verification 2. 0 should be to make network verification tools as prevalent as ping and traceroute. Just as no complex hardware or software artifact is built today without rigorous testing, no network should be built without rigorous verification. Meeting this goal requires addressing different challenges than the ones we have addressed thus far. Network Verification 1. 0 addressed two challenges 1) formulating the verification problems in a way that balanced utility (find interesting bugs) and tractability (in a reasonable amount of time), and 2) developing analytical techniques to efficiently solve those problems. These techniques are sufficiently powerful that they are not what is holding back network verification from mainstream adoption. Instead, what impedes adoption today is how easily network engineers can use the tools and how easily tool authors can extend analysis to adjacent functionality not already covered such as link layer protocols, virtual networks, etc. Thus, Network Verification 2. 0 must address these challenges: 1) making it easy for network engineers to consume network verification; and 2) making it easy to extend verification tools to cover more network functionality. We explain these challenges below and outline a solution approach inspired by the domain of software analysis. The challenge of consuming network verification: Verification is difficult to consume for network engineers today because its input-output abstractions as well as goals are different from what they are familiar with. The input to common tools such as traceroute is a concrete flow (i. e. , a starting point and packet headers). In contrast, verification inputs are (large) sets of flows. Precisely specifying such sets is hard because of the unwritten assumptions inside many real networks. Consider the task of verifying that “no packet from A to B is dropped. ” A natural way to verify this behavior is via the invariant that all possible packets that start at A, with B as their destination address, reach B. However, this invariant can legitimately fail in a network that is configured to prevent source IP spoofing, or prevents packets for certain blocked applications (e. g. , telnet), or blocks packets to/from IP addresses that are deemed malicious by an external service. The precise invariant that engineers need to specify is thus complex and possibly also time-varying. We need to make it easier for engineers to specify such invariants. Similarly, the verification tool output may be large sets of flows that are dropped from A to B. Communicating such sets, which may be a complex combination of multiple header fields, is tricky. For example, the path from A to B may contain a number of access control lists (ACLs) similar to the ones shown below, the ACLs may have 100s of lines, and may filter on more than just the source and destination addresses.  Communicating complex spaces via (a few) examples is a possibility but can be misleading. If we pick a source IP that is not in the network, the engineer may conclude that the discovered violation is irrelevant. In reality, the source IP was just an example and in-network IPs (not picked as an example) may also lead to invariant violations. We need to fundamentally rethink how the complex output of verification tools can be effectively communicated to engineers. Finally, consider the goals of common tools today versus verification. While traceroute is often used to diagnose reported problems, verification is intended to proactively ensure the absence of problems. When diagnosing reported problems, the engineer knows when their job is done. However, when proactively protecting against (unknown) problems, how do they know when they are done? Verification is not an on/off switch, and verified networks can fail too. We have seen incidents where networks that use verification still suffered outages despite the bug being detectable via verification. The engineers did not have an invariant in their verification test suite that would have flagged that bug. To prevent such outages, engineers need feedback on how well they are using network verification and help them use it more effectively. The challenge of extending network verification tools: Network verification tools are engineered as monoliths today. They have a custom model of the target network functionality (e. g. , packet forwarding) and an analysis engine that is intimately tied to the model. This coupling makes it difficult to extend network verification to more functionality that we may know how to verify analytically but the tool does not support (e. g. , packet forwarding with a new type of transformations or virtual networks with underlay and overlay forwarding). Such extensions require one to develop or modify both the model of network functionality and the analysis engine. This is a tough task because of the amount of work and the broad expertise needed in networking as well as formal methods. This difficulty makes verification hard to employ in networks where key functionality is even slightly different from the ones targeted by an existing tool. We have witnessed this difficulty first hand with tools that we have helped develop. At Microsoft, our engineers have developed tools to verify the safety of changes to access control lists, network security groups, and forwarding table rules. Each time we had develop a new tool from scratch despite the similarity of these functionalities because extending the prior tool was no easier than building a new one. Similarly, extending Batfish over the years to support more network functionality or additional router vendors with slightly different semantics has required significant changes to the analysis pipeline. The monolithic approach was helpful for the first generation of network verification tools because the tight integration between network functionality and analysis enabled faster iteration. However, this tight coupling is no longer necessary and we can decouple network modeling from analysis. Network functionality can be modeled in a common language and analysis engines can target the language instead of specific functionality. That way, extending verification to new functionality only requires modeling that functionality. Once that is done, one or more analysis engines become available to analyze that functionality. A linguistic approach: There are multiple ways to tackle the Network Verification 2. 0 challenges above, but a linguistic approach is particularly appealing (which we are exploring; see here and here). To make it easy for engineers to specify inputs, along with their networks’ unwritten assumptions, we should design an invariant language that can naturally express sets of values and set operations like unions and differences. We may consider a language that blends SQL-like queries over a data model of the network and regular expressions over path constraints. A high-level invariant language over a well-defined data model also makes it easy to track which network elements (e. g. , devices, interfaces, paths) are “covered” by an invariant. This basic capability can then be the basis for providing feedback to network engineers about how well their invariant set covers the network. This is similar to how test frameworks for software can quantify the extent of software coverage, though we will need custom measures for network coverage given the differences in the two domains (e. g. , a network is not a sequence of statements or basic blocks, which is the model employed by software test frameworks). Finally, we should look toward intermediate verification languages such as Rosette, Boogie, and Kaplan to engineer network verification tools to be easy to extend. These languages allow multiple source languages to be compiled to them and various analysis approaches then become accessible, without the need for inventing source language specific analysis tools. In a similar manner, we could translate different network functionalities into an intermediate language and develop a common set of analysis tools for this language. Then, verification can be easily extended to any functionality that can be translated to this intermediate language. Of course, for this approach to be practical, we need to design an expressive intermediate language for which engineers can develop efficient analyses. Summary: The next generation of network verification research must enable broad use of verification tools such that all networks are rigorously analyzed and they become indispensable to engineers like ping and traceroute are today.  We can achieve this goal by making the tools more accessible to engineers on the ground and easy to extend to adjacent functionality.  Solving these challenges will radically improve the security and reliability of the networking infrastructure that is critical to modern society. "
    }, {
    "id": 24,
    "url": "http://localhost:4000/the-verification-synthesis-spectrum/",
    "title": "The Verification-Synthesis Spectrum",
    "body": "2020/08/31 - (Sung to the tune of “The Big Bang Theory” theme song by the Barenaked Ladies)Our whole network was in a simple, dense stateThen nearly eighteen thousand days ago expansion started, waitThe networks were complex, the engineers were quite perplexed,Then researchers developed toolsThat can verify (That can synthesize)Verification, synthesis, which of these are best for this?That’s the focus of this article! Hey! In recent years, researchers have developed a plethora of tools for verifying and synthesizing router configurations. Configuration verifiers analyze a snapshot of a network’s current or proposed configurations to determine whether the network satisfies all forwarding policies (e. g. , reachability, isolation, path preferences, etc. ) under various conditions (e. g. , link/node failures). Configuration verifiers output a list of (concrete) policy violations to alert network engineers to potential errors in the configurations. In contrast, configuration synthesis tools automatically produce brand-new configurations that satisfy a network’s forwarding policies. The synthesized configurations are guaranteed to be error-free—or “correct-by-construction” in formal methods terms. From a formal methods perspective, verification and synthesis are closely intertwined. First, they are both search problems: verifiers search for a proof of configuration correctness, and synthesizers search for a correct configuration. Second, they both require a model that encodes the semantics of configurations and routing algorithms. Finally, they both require a specification that encodes a network’s expected forwarding behavior, or intents. But from a practical perspective, verification and synthesis are quite different. Verifiers operate on existing configurations, whereas synthesizers create configurations. Verifiers find errors, whereas synthesizers avoid errors. These differences make verification the best choice in some situations and synthesis the best choice in other situations; we discuss this in more detail below. However, these extremes aren’t the only option. Some tools, including our own research, strike a middle-ground between verification and synthesis. These configuration repair tools are designed to uncover violations of network policies and automatically produce configuration updates that eliminate the errors. The Verification-Synthesis Spectrum: As discussed above, configuration verification and configuration synthesis are closely intertwined but satisfy different goals. Moreover, different verifiers provide different amounts of information, and different synthesizers target different types of configurations. This prompts us to view configuration verification and synthesis as a spectrum.  On the far left are configuration verifiers that take a networks’ current configurations and expected forwarding behaviors as input, and produce a simple yes/no—i. e. , satisfied/violated—answer for each forwarding requirement: e. g. , Tiramisu and ERA. On the far right are configuration synthesizers that require only a specification, and produce configurations entirely from scratch: e. g. , Propane and SyNET. However, some verifiers provide more than a simple yes/no answer for each forwarding requirement. For example, Minesweeper and ARC both produce a concrete counterexample—e. g. , a failure scenario and resulting forwarding path—for each violated requirement. Going a step further, Batfish and Plankton produce complete forwarding information bases (FIBs), and Batfish tracks the provenance of each FIB entry, such that a network engineer can trace the sequence of events and set of configuration statements that led to the FIB entry’s existence. Similarly, not all synthesizers produce configurations entirely from scratch. For example, Propane/ATsynthesizes abstract templates based on device roles (e. g. , core andborder routers) and generates/updates concrete configurations as thetopology grows. Similarly, Zeppelin allows a network engineer to specify policies regarding configuration structure—e. g. , which routers should run BGP? how many OSPF areas should there be?—which inform the structure of the synthesized configurations. Going a step further, NetComplete requires network engineers to provide a configuration template with holes for key parameters—e. g. , link costs, route filter match criteria, etc. —and synthesizes values for these holes, rather than the complete configuration. (We’ll discuss the middle of the spectrum, which includes CPR and Jinjing, later in this article. ) It is important to note that this spectrum only captures some of the similarities and differences between various tools. Verifiers and synthesizers also differ in terms of the protocols/features they support, the environment variables they consider (e. g. , link/node failures, external route advertisements), how well they perform/scale, etc. Figure 1 in the Minesweeper paper provides a more nuanced overview of the network verifier landscape. Additionally, the spectrum does not include data plane verifiers—e. g. , HSA, VeriFlow, and Delta-net—data plane synthesizers—e. g. , Genesis—or configuration anomaly detectors—e. g. , rcc, Minerals, and SelfStarter, because these tools are fundamentally different from the configuration verifiers and synthesizers included in the verification-synthesis spectrum. In particular, data plane verifiers and synthesizers ignore a network’s configurations, and configuration anomaly detectors ignore a network’s forwarding requirements. Choosing a point in the spectrum: The spectrum of configuration verifiers and synthesizers that exist today raises a fundamental question: which point(s) in the spectrum should a network practitioner choose? One way to approach this is to ask a slightly different question: are you working with a greenfield or a brownfield network? A greenfield (i. e. , brand new) network is a perfect opportunity to employ configuration synthesis, because a greenfield network does not have any existing configurations. There may be configuration templates that were used in other (similar) networks, which could motivate choosing a partial configuration synthesizer (e. g. , NetComplete) as opposed to a complete configuration synthesizer (e. g. , Propane and SyNET). In contrast, network verifiers are often better suited for a brownfield (i. e. , partially existing) network, because configurations already exist, changes are often small, and network engineers already have established network management practices. (These observations are supported by studies of configuration changes in large university campus networks and large data center networks. ) A different way is to ask: how much automated change are you willing totolerate? Clean-slate configuration synthesizers (e. g. ,Propane andSyNET) completelyignore a network’s current configurations and synthesize brand-newconfigurations. This requires wholesale replacement of a network’sconfigurations each time forwarding policies change, which is a risky andpotentially disruptive operation—e. g. , network engineers from a regionalresearch and education network reported that flash reliability problems cause some routers in their network to crash when the configuration isupdated. Moreover, any hand-tuning of configurations may be wiped out. Partialconfiguration synthesizers (e. g. ,NetComplete andPropaneAT)allow some aspects of a network’s configurations to remain fixed. However,it’s up to network engineers to determine how much of a configuration tomanually specify (in a template) and how much to synthesize. More manualspecification means more work for network engineers and a relaxation of the“correct-by-construction” guarantees provided by configuration synthesizers. In contrast, configuration verifiers are “read-only” insofar as they flagconfiguration errors but do not generate any part of the configurations. Thisallows network engineers to have complete control over configuration changes,which enables engineers to hand-tune changes according to existing networkmanagement practices and preserve their networks’ configuration “style. ” A middle ground: configuration repair: It is hopefully clear from our discussion thus far that configuration verifiers and synthesizers each have their merits. However, an important question remains: is there a middle ground? Is it possible to obtain the benefits of synthesis for a brownfield network? Is it possible to automate some changes? Fortunately, the answer is yes. At the middle of the verification-synthesis spectrum are tools designed for configuration repair: e. g. , CPR and Jinjing. These tools can automatically find and correct errors in hand-written changes and/or automatically update configurations to satisfy new forwarding requirements. Unlike configuration synthesizers, configuration repair tools do not generate new configurations from scratch. Rather, configuration repair tools identify a minimal set of changes that must be made to the current/proposed configurations to ensure the network satisfies all forwarding policies. Moreover, unlike configuration verifiers, configuration repair tools do not leave the task of correcting errors as an exercise for network engineers. Rather, configuration repair tools automatically address forwarding policy violations. Existing configuration repair tools have demonstrated the merits of this middle ground, but a lot of work remains to be done. In particular, CPR prefers changes that modify the fewest lines of configuration, but the smallest change is not always the best change. For example, enabling route redistribution is a single-line change, but route redistribution is notoriously difficult to reason about. Instead, a multi-line change that adds new neighbor relationships and applies appropriate filters may be easier for network engineers to reason about and better align with a network’s current configuration “style. ” Additionally, CPR and Jinjing are limited in the types of configurations they can handle: CPR only handles static routes, OSPF, and a limited subset of BGP, while Jinjing only handles ACLs. In contrast, existing configuration verifiers and synthesizers can handle a wide range of protocols and features. In our own ongoing work, we are working to address these shortcomings (and hope to write a future article about this). We hope others will do the same. "
    }, {
    "id": 25,
    "url": "http://localhost:4000/aalwines/",
    "title": "Toward Polynomial-Time Verification of Networks with Infinite State Spaces: An Automata-Theoretic Approach",
    "body": "2020/07/20 - With the increasing scale of communication networks, failures (e. g. link failures) are becoming the norm rather than theexception. Given the critical role such networks play for ourdigital society, it is important to ensure a reliable andefficient operation of such networks, even in the presenceof one or multiple failures. While several interesting automated approaches to verify and operate networks are emerging, offering an attractivealternative to today’s pragmatic and manual “fix it when it breaks” approach, existing solutions often only provide a limited and inefficient support for reasoning about failurescenarios. In particular, verifying networks is a complex task, even for computers. For example, many existing automated solutions require to test each possible failure scenario (respectively, each possible dataplane) individually, resulting in a runtime which can be exponential in the number of failures; which makes this approach impractical for large-scale communication networks. In fact, certain communication networks, such as networks based on MultiPath Label Switching (MPLS) or Segment Routing (SR), may even have a theoretically infinite configuration space: in MPLS and SR networks, default routes as well as failover routes depend on the label stack in the packet header, which may grow and shrink arbitrarily along the route. In this article, we argue that automata-theoretic approaches can offer an interesting solution to these problems, allowing to perform fast verification, accounting even for failure scenarios and dealing with potentially infinite state spaces. A case for verifying network configurations: Before delving into the details, we first argue that it is often attractive to verify thenetwork configuration directly: compared to verifyingcontrol plane protocols, analyzing the dataplaneconfiguration (e. g. , forwarding and conditionalfailover tables) has the advantage that italso accounts for possible external changes,e. g. , manual changes through the CLI, bugs, or changes made byadditional protocols. Moreover, dataplane-based approaches can also reveal problems in the network operation introduced by the algorithmsthat generate the routing tables from the control plane specifications. Let us consider the verification of forwarding rules in MPLS routing tables as an example. In a nutshell, in MPLS, packet labels can be nested in order to provide tunneling through the network or to handle linkfailures by the fast-reroute mechanism. This mechanism relies on pushing a new MPLSlabel on top of the label stack, to redirect the flow to go around the failed link. The mechanism can be applied several timesin case of multiple link failures, creating larger and larger (in theory: unbounded)numbers of MPLS labels on the label-stack. A fairly general query language which supports to reason about network behavior under failures could be based on regular expressions: $ &lt; a &gt; b &lt; c &gt; k $ Here $a$ and $c$ are regular expressions over MPLS labels that describe the set of allowed initial resp. final headers of packets in thetrace, $b$ is a regular expression over the links in the network, defining the setof allowed routing traces through the network, and $k$ is a numberspecifying the maximum number of failed links to be accounted for. By using regular expressions, we can test properties such as waypoint enforcement (e. g. , is thetraffic always forwarded through an intrusion detection system)or avoidance of certain routers in selected countries. How does polynomial-time what-if analysis work?: In order to evaluate such queries and conduct what-if analysis quickly, we make the case for a model-checking algorithm which is based on automata theory. Our approach is motivated by a natural (but not well-known) connection to the theory of pushdown automata and prefix rewriting systems. In a nutshell, given the network configuration, the routing tables,as well as the query, a pushdown automaton (PDA) can be constructed. On this PDA, we can then perform a standard reachability analysis. In more details,the regular expressions for the initial header and the finalheader of the query can each be converted to first aNondeterministic Finite Automaton (NFA) and then to a PushdownAutomaton (PDA). The path query is converted to an NFA, which can then be used to restrict (by synchronous product)the behavior of the PDA constructed based on the network model. The three PDAs can be combined into a single PDA that can be givento the backend engine that checks the emptiness of the PDA language. In case the language is nonempty, we can return a witness traceto demonstrate the reason why the query is satisfied. All these operations can be performed in polynomial time, leveraging a classic result by J. Büchi. The AalWiNes Tool and Example: As a case study and to explore the feasibility of such an automata-theoretic approach, we developed a framework, the AalWiNes suite, which allows to load an arbitrary network topology (e. g. , from the topology zoo dataset) as well as a set of router configurations (e. g. , from Juniper routers). The configurations are put into a standardized, intermediate format which is vendor independent. The user can then define a query as described above, which leads the tool to prompt with a result (e. g. a trace). AalWiNes is a joint work at Aalborg University and the University of Vienna. For MPLS specifically, we have devloped the AalWiNes tool suite, implemented in C++, and improving upon an earlierprototype implementation in Python [1]. The tool allows to verify a wide range of important network properties in polynomial time, parameterized by the $k$. Here is a quick intro to AalWines, using an interative web-browser integration of our tool available at demo. aalwines. cs. aau. dk: AalWiNes also allows to account for more complex traffic engineering aspects, such as load-balancing, by supporting nondeterminism, as well as more complex multi-operation chains. The tool includes several optimizations to further improve the performance, such as “top of stack reduction”,which safely calculates which labels can be at the top of stack in a given state of the PDA: the top of stackreduction technique greatly reduces the amount of transitions in the PDA. We refer to our tool website as well as to our project website. Conclusion and future work: Our aim is to provide a low runtime which is critical for the success of networkverification, however, reasoning about failures seems particularly challenging with respect to the computational complexity. While not every network may allow for a polynomial-time analysis,we see much potential in automata-theoretic approaches,and we hope this article can inspire the community to conduct more research in this area. We understand our approach as a first step and believe that it opens the door to verify many other infinite state systems whose configurations cannot explicitly be represented anymore. For example, we have recently extended AalWiNes to support a witness trace generation which accounts for additional metrics; for example, the tool can now select (among a possibly infinite number of witness traces) the ones that minimize the number of hops, latency, or the stack height that corresponds to the number of MPLS tunnels. This isachieved by extending the PDA reachability analysis with multi-dimensional weights. Another interesting avenue for future research isthe study of similar approaches for control plane verificationor for supporting capacity planning, also taking into accountresource constraints on failover paths. References: [1] : P-Rex: Fast Verification of MPLS Networks with Multiple Link Failures. Jesper Stenbjerg Jensen, Troels Beck Krogh, Jonas Sand Madsen, Stefan Schmid, Jiri Srba, and Marc Tom Thorgersen. 14th ACM International Conference on emerging Networking EXperiments and Technologies (CoNEXT), Heraklion/Crete, Greece, December 2018. "
    }, {
    "id": 26,
    "url": "http://localhost:4000/talk-to-a-network-operator-today/",
    "title": "Talk to a network operator today!",
    "body": "2020/07/05 - Recently, we have seen significant advances in tools that bring formal methods to networking. These tools verify if a given network satisfies important properties, automatically repair broken networks, and synthesize networks that provably satisfy properties. The hope is that these tools will help turn the practice of networking from a complex, potentially error-prone process to a principled discipline that is rooted in sound science and engineering, resulting in networks that are less bug-prone and, ideally, bug-free. While we have made big intellectual strides in developing these tools, it still feels like the beginning of a long journey. In our view, the research community as a whole has a ways to go before the tools are useful and widely applicable. And a major reason is the lack of close interaction with the network operator community. This article is a call to arms to those researching in this space to have a“network operator-buddy”, actually, several operator-buddies. Talk to thesebuddies about their needs and pain-points; talk to them about obtaining accessto operational data, and let the data guide your research as much as possible;talk to them about your ideas, and get them to use your tools! Our experiencehas been that this opens up not only interesting new research problems butalso a direct path to real-world impact. Such conversations can be especiallyvaluable for newcomers to this space. Below we outline some highlights from our own exploration of this research topic, and how it was, and continues to be, largely informed by feedback and help from network operators and by insights derived from their invaluable data. How data, and conversations with operators, opened the door for us: Early work in this space focussed on network dataplane verification; notableexamples include Anteater and HSA. These works aimed to validate if the current forwarding state of the network satisfies important properties, such as ensuring a given pair of hosts are blocked, and that there are no routing blockholes or forwarding loops. Around roughly that time, we were engaged in an empirical project onunderstanding operational managementplanes – howreal-world networks are designed, and operated. To this end, we collectedoperational data (longitudinal network configurations, trouble tickets,topologies) from various networks around the US, including cloud data centers,enterprise networks, and university campus networks. (Data is also openlyavailable from various research and education networks, includingInternet2and Purdue University. )A key component of this study was the frequency of configuration changes. Ourstudy shed light on a surprisingly high frequency of change events, where eachchange event includes updates to configurations of multiple network devicesclose to each other in time. We found that change events can be of variousscopes, some spanning an entire network, and others touch just a couple ofdevices. Crucially, we found critical empirical evidence that networks with a greater degree of change – either frequent changes, or changes that are big in scope, or both – were more susceptible to failures (as indicated by higher incidence of trouble tickets). This in particular hinted at a potentially important use case that the state-of-the-art dataplane verification tools may be missing, namely, proactive verification. Dataplane verification tools verify just the current snapshot of the network’s forwarding state. Given the frequency of changes, and the relationship between changes and outages, we felt that network operators may also be interested in whether the modifications might result in violations of properties. Our informal conversation with operators from US Big-10 schools (of which UW-Madison is one) confirmed this for us: operators indicate that they would much rather analyze configuration changes before deployment, rather than identify bugs when they manifest after deployment, by which time it is often already too late! About the same time, a group of researchers presented Batfish – the first control plane verifier to ``derive the actual data plane that would emerge given a configuration and environment. ’’ Using Batfish, the researchers uncovered a variety of bugs in two real university networks. Our own and others’ analysis of operational data and discussions with operators thus opened the door to our work onARC andsubsequently, Tiramisu, tools for proactive verification of network control planes. More conversations helped us hone in on our approach: Early on when developing a framework for proactive control plane verification,our focus was largely on “one-shot” verification of properties, such as verifying if a given control plane configuration allows two hosts or subnets to reach each other, or that the configuration results in no routing blackholes. However, UW-Madison operators anecdotally mentioned several incidents where their network’s configurations appeared safe (i. e. , passed various simplistic tests) prior to rollout, but the network nevertheless experienced major service outages under unexpected but routine events such as a link or a switch failure. These anecdotes highlighted for us the importance of analyzing a control plane’s ability to satisfy key properties under various possible failure scenarios, i. e. , being able to answer questions such as: does this configuration change continue to ensure reachability between a given pair of subnets even when an arbitrary set of k links fails unexpectedly? This observation led to a major change in our approach to verification: wewere initially exploring a simulation based approach (similar to Batfish) where we would execute a control plane’s processing to determine what paths it might end up generating, but such an approach becomes quickly untenable when exploring the large number of failure scenarios – running a simulation (which is already slow) per failure scenario is prohibitively expensive. We then pivoted to develop a control plane model that is amenable to rapid simultaneous exploration of many failure scenarios. The result was ARC, a first-of-a-kind graph based control plane encoding. ARC allows mapping the problem of proactively verifying if a control plane satisfies a given property under failures (e. g. , “k-reachability” as exemplified above) to running appropriate fast, polynomial time graph algorithms (e. g. , determining if the graph min-cut &gt; k). In trying to enable fast analysis under failures, ARC makes some assumptionsabout the control plane’s design. For example, it assumes that routingprotocols such as BGP are used in specific ways (such as, no communityattributes, and no iBGP), and layer-2 (e. g. , VLAN) configurations are correct. Talking to operators from NYSERNET,ESnet, UW-Madison, and ColgateUniversity, showed us that these assumptions arequite limiting in practice and hinder ARC’s applicability. Analyzing theconfiguration data from some of these networks further showed the broad extentto which the protocol features we “assumed away” were in use. This analysisled us to Tiramisu, our most recent verification tool; Tiramisu’s controlplane embedding builds off of ARC’s simple graph model. Notably, Tiramisu usesa multi-layer graph to encode inter-protocol dependencies (e. g. , dependencies between BGP and IGPs, and routing protocols dependence on layer-2    technologies like VLANs), and multi-dimensional edge attributes toencode the various metrics routing protocols use in their decision-making. (Stay tuned for a future article where we hope to cover Tiramisu’s technical underpinnings in more detail!) Identifying new directions: Operators continue to offer us valuable insights on our verification toolsthat have steered our work in an interesting new direction, and helped usidentify which problems are the most important to work on. Notably, at a presentation on ARC and Tiramisu at a large online serviceprovider, a network operator asked us if the graph-based control planeabstractions underlying these tools can be used to validate if a given networkis susceptible to overload: more precisely, does there exist a combination ofa group of links failing and a network traffic matrix that causes thenetwork’s control plane to select paths that may overload certain links? Thisis an example of a “quantitative verification question” that existingproactive verification tools cannot answer because of their exclusive focus onqualitative path properties (such as path existence, number of paths, etc). Interestingly, the same question arose in a discussion we had with operators at UW-Madison. This led to our work on QARC, a tool for exhaustively checking networks for potential susceptibilities to overload.  We hope to talk about QARC at length in a future article. Our work on control planerepairand synthesis – yet another topic we hope to explore in a future article – were also both heavily influenced by surveys of network operators, analysis of operational data, and, of course, conversations and interviews with operators. How to make operator-buddies?: As our experience illustrates, operator-buddies can be an invaluable resource. They can help understand what problems to focus on, identify how to refine candidate approaches, and also suggest new directions altogether. But how does one (especially someone in an academic setting) go about making operator-buddies? Luckily in our experience there are plenty of avenues for building relationships with network operators. First, consider setting up a coffee meeting or two with your campus operators. You will be surprised at how readily willing they are to meet, talk abouttheir operational practices and pain points, and also share data! (If they are    hesitant to share raw data with you, you can ask if they could share    configurations that have been anonymized using tools like Netconan. ) Once you have built a sufficiently close relationship, your campus operators may also be willing to introduce you to their friends from other regional schools or networks who themselves may contribute more discussion or even data. Also, consider spending time in the industry, e. g. , as part of an year-long sabbatical or summer internship, and work closely with network operators there. You will likely obtain access to data while employed there and you may lose access when you leave, but the insights you gather will stay with you and inform your work for years to come. Finally, consider attending network operator events such asNANOG and CHI-NOG, or conferencesco-located with events such as the Open NetworkingSummitand IETF meetings, and setting up conversations with network operators there. These events attract people running a very broad variety of networks, campuses, enterprises, ISPs of various sizes, and cloud operators, giving you perhaps the broadest exposure to your ideas and the broadest possible net for your to rope in operator collaboration. Being a passive observer of the discussions within these groups can also be insightful, so sign-up for their mailing lists. Or, drop us a note! We would be happy to put you in touch with our local operator-buddies and you can take it from there! "
    }, {
    "id": 27,
    "url": "http://localhost:4000/models-of-distributed-protocols/",
    "title": "Models for Distributed Routing Protocols",
    "body": "2020/06/22 - Part 1: SIMPLE Models and SimulationOne of the keys to success of any verification effort is identifying a class of models capable of representing the important elements of the system under consideration.  Such models inevitably elide information—the real world is too complicated to represent it in its entirety.  However, a good model represents enough of the real world to be useful.  When it comes to software reliability, such models need to represent the causes of important bugs or vulnerabilities. For instance, relational algebra has been an effective model for database query languages for decades.  It represents database tables as relations, which, mathematically speaking, are just sets of tuples, and is capable of defining a variety of standard database operations such as filters, joins and maps.  It has been indispensable for helping database implementers define and prove correct complex query transformations and optimizations.  Of course, there are infinitely many things that the (standard) relational algebra models will not capture about databases: the concrete syntax of user queries, the cost of computing filters or joins, or the semantics of the underlying storage system and its potential for failure, to name just a few. The networking community, like the databases community, has its fair share of models as well.  In this post, we will introduce a simple class of models that capture the essence of distributed routing protocols like eBGP, OSPF, RIP and ISIS.  They describe the flow of routing messages between devices and allow us to analyze the routes (i. e. , paths) computed by the network control plane, without getting bogged down by the bells and whistles of real-world protocols. The bells and whistles are important for practical reasons but have little bearing on a broad class of verification questions of interest such as: Does the network control plane compute a route from server A to server B? Might Google accidentally try to act as transport for traffic destined for Japan, possibly shutting off access to the internet there? Will Level 3 leak routes that should be kept internal, disrupting internet service across the US? By developing the capability to analyze these simple models, we can identify a range of potentially devastating errors in the configuration of modern networks. The models we will discuss here are derived from the pioneering research of Griffin et al. [1] and Sobrinho [2].  Griffin identified the fact that network protocols like BGP are solving a kind of “stable paths problem”.  In doing so, he developed a solid framework for thinking rigorously about such protocols and analyzing their basic properties, including convergence and determinacy.  Sobrinho generalized these ideas and formalized them from an algebraic perspective.  More recently, we developed NV [3, 4], a language and system that allows users to define and verify such models using powerful logical tools. SIMPLE: An Idealized Control Plane Protocol: While there are many distributed control plane protocols, they share a common structure.  That common structure allows us to define a wide range of protocols in the same way, and makes it possible to implement generic analysis tools that can be reused to find bugs in any of them. To illustrate the basic pieces of a model, let’s take a look at a concrete example—a made-up protocol we will call SIMPLE.  In SIMPLE, each router in the network either knows a route to the destination or it doesn’t.  We use the symbol None to represent “no route. ” Each known route is a triple (pref, length, next).  The first component, pref, is a numeric preference for the route.  The higher the number, the more desirable the route.  In general, the reader can assume the protocol uses 32-bit numbers that range from $0$ to $2^{32}-1$, but the details do not matter.  The second component is the length of the path to the destination.  The third component identifies the router that serves as the next hop along the journey to the destination.  For simplicity, SIMPLE is designed to route to a single destination, so we don’t need to include the name of the destination (i. e. , its IP address) in the routing messages.  We’ll discuss more elaborate models for multi-destination routing later in this series of articles.  To summarize, we have now defined the first component of a control plane model, the set of messages M, that are used to disseminate routing information amongst routers. Below is an example of a network running SIMPLE in an initial state, before the process of computing routes to the destination has begun.  This network has 5 routers, named $\mathtt{R_0} - \mathtt{R_4}$.  Each router is annotated with an initial route.  $\mathtt{R_0}$ is the destination router—its initial route has a preference of 100, a length of 0 to the destination (it is the destination), and a dummy next-hop of 0.  The other routers have None as their initial route.  In general, the current state X of one of our network models is a function from routers to their current route.  The initial state init, is an example of such a state.  In this case, we define the init state for this network as follows: $\mathtt{\mathbf{init}_{R}} = \mathtt{(100, 0, 0)}~~~,~\mathrm{if}~\mathtt{R} = \mathtt{R_0}$ $\mathtt{\mathbf{init}_{R}} = \mathtt{\mathit{None}}~~~,~\mathrm{otherwise}$ The routing process for SIMPLE, and other protocols we model, operates by transmitting messages around the network, and updating the network state as we go.  Eventually (we hope), the process stabilizes and no more state changes occur. Informally, SIMPLE operates as follows.  Each router with a valid route can choose to export the route to one or more of its neighbors.  When a message traverses an edge in the graph, it is transformed. The length of the route will increase by one, the next hop field will change, and the importer will change the preference field, making the route more or less desirable.  We call the function that executes those transformations the trans function.  Since the transformation may vary from one edge of the graph to the next, when we care to be specific, we write $\mathbf{\mathtt{trans}}_{\mathtt{e}}$ for the transformation function to be applied across the edge e. When a router receives one or more routes from its neighbors, it compares those routes with its initial route and chooses the most desirable route available.  More specifically, it prefers the route with the highest preference value.  If there is a tie, it will prefer the route with the shortest path length.  If there is still a tie, it prefers the route through the lowest numbered neighbor (e. g. , all other things being equal, router $\mathtt{R_3}$ prefers routes it receives from $\mathtt{R_1}$ over those it receives from $\mathtt{R_2}$).  In general, when multiple routes are available at a router, the router will use information from those routes to compute its most desired route.  We call the function that executes that computation the merge function, and often write $\mathtt{R_1~+~R_2}$ to denote the merge of two routes, $\mathtt{r_1}$ and $\mathtt{r_2}$.  In principle, the merge function can differ at every router.  If we care to distinguish between the merge functions of particular routers, we will write merge$_R$, or $\mathtt{R_1}~\mathtt{+_R}~\mathtt{R_2}$, for the merge at router R, but to keep the notation light, we will often assume the merge functions are the same across all nodes and elide the subscript. Choices such as which router forwards routes to which other routers, and how to change a field like the preference value are typically part of the configuration of a protocol.  In contrast, the notion of “most desirable route” (i. e. , the merge function) is usually a fixed part of the protocol.  The configurations are typically defined by network operators—they are what allows a protocol to be customized to the needs of a particular network.  Router vendors like Cisco and Juniper have complex, proprietary languages for configuring their devices, and in large networks, such configurations can be hundreds or thousands of lines of code per router, and hundreds of thousands or millions of lines in total for a large data center network.  However, when it comes to network verification, it does not really matter whether a route processing function is defined by a configuration or is an inherent, unchangeable part of the protocol.  Hence, our models incorporate both and do not distinguish between fixed and configurable parts. In the following picture, we add a little bit of configuration information to our network.  In particular, we will assume that when $\mathtt{R_3}$ imports a message from $\mathtt{R_1}$ it changes the preference value to 0, making it less desirable than routes imported from $\mathtt{R_2}$. We can also configure the network so that certain links drop messages.  For simplicity, we will assume links propagate messages from right to left, and drop all messages (i. e. , producing None) from left to right.  Protocol Simulation: To determine which paths each router in the network chooses to use, we can simulate execution of the control plane protocol on the given network.  Simulation updates the network state, one step at a time until a “stable state” is found and no more updates are required.  The following (non-deterministic) algorithm implements the simulation process. Generic Simulation Algorithm:    Let X, the current state of the simulation, be the initial state init.     Select any router R:      Let $\mathtt{R_1} \ldots \mathtt{R_k}$ be the neighbors of R   Let $\mathtt{e_1} \ldots \mathtt{e_k}$ be the edges that connect those neighbors to R   Update X(R) as follows (leaving other components of X unchanged):         $\mathtt{X}(\mathtt{R}) := \mathbf{\mathtt{trans}}_{\mathtt{e1}}(\mathtt{X}(\mathtt{R_1}))~\mathtt{+_R}~\ldots~\mathtt{+_R}~\mathbf{\mathtt{init}}(\mathtt{R})$          Repeat step 2 until there are no further changes to be made. i. e. , until $\mathtt{X}(\mathtt{R}) = \mathbf{\mathtt{trans}}_{\mathtt{e1}}(\mathtt{X}(\mathtt{R_1}))~\mathtt{+_R}~\ldots~\mathtt{+_R}~\mathbf{\mathtt{init}}(\mathtt{R})$for all routers R In our SIMPLE network, a possible first step in simulation could select router R1 and consider its neighbors, $\mathtt{R_3}$ and $\mathtt{R_0}$.  The current chosen routes for those routers are None and (100, 0, 0) respectively.  The initial route at $\mathtt{R_1}$ is None.  Hence, the new route chosen by $\mathtt{R_1}$ after this step in simulation will be: $\mathtt{\mathbf{trans}_{01}}(100,0,0) ~\mathtt{+}$ $\mathtt{\mathbf{trans}_{31}}(\mathtt{None})~\mathtt{+}$ $\mathtt{None}$ which is equal to the following (we add 1 to the length of the known route and leave the other fields unchanged): $(100, 1, 0) ~\mathtt{+}~ \mathit{None} ~\mathtt{+}~ \mathit{None}$ which is equal to $(100, 1, 0)$ as the SIMPLE protocol always prefers some route over None.  The following picture diagrams the process.  The green route is the route that emerges for $\mathtt{R_1}$ after a single step of simulation: Cleaning up the picture, our network now looks like this: Next, it may be $\mathtt{R_3}$’s turn to act.  As mentioned above, when $\mathtt{R_3}$ imports its message from $\mathtt{R_1}$, it downgrades the preference to 0.  However, since $\mathtt{R_3}$ only receives None (no route yet) from $\mathtt{R_2}$, it prefers the low-preference route from $\mathtt{R_1}$ over no route at all. After $\mathtt{R_3}$, $\mathtt{R_4}$ may take a turn, leaving the network in the following state after those two steps: And then the simulation might choose to update $\mathtt{R_2}$: And now you will notice that all routers have a valid route (none of the routers have None as their route).  Still, the simulation is not complete.  If router $\mathtt{R_3}$ goes again at this point, it will compute the new route (100, 2, 2), which differs from its current route of (0, 2, 1).  It does so because SIMPLE prefers routes with higher preference value.  As a result, $\mathtt{R_3}$ prefers the route it receives from $\mathtt{R_2}$ (with preference 100) rather than the route it received earlier from $\mathtt{R_1}$ (with preference 0).  $\mathtt{R_3}$’s next hop is now 2 instead of 1.  And then finally, we need to consider $\mathtt{R_4}$ again.  When $\mathtt{R_4}$ pulls routes from all its neighbors now, we arrive in the following state.  Model Solutions: At this point, we can examine all routers R and check to see whether the current chosen route equals $\mathbf{\mathtt{trans}}_1(\mathtt{r_1}) \mathtt{+_R} \ldots \mathtt{+_R} \mathbf{\mathtt{trans}}_k(\mathtt{r_k}) \mathtt{+_R} \mathbf{\mathtt{init}}(\mathtt{R})$ where $\mathtt{r_1}$ through $\mathtt{r_k}$ are the current routes of R’s neighbors.  It turns out they all do. (Aside: Recall that we specified that edges, when traversed from left to right, drop all routes—that is, they convert any route from left to right into None—and so the route with higher local preference of 100 at $\mathtt{R_3}$ does not propagate backwards to $\mathtt{R_1}$.  If we did not block this route, the system would not yet be stable. ) So we’ve reached a stable state of the system, which we also call a solution to the routing system.  More formally, a solution L is any state of a network that is globally stable in the sense that all routers R satisfy the following equation: $\mathbf{\mathtt{L}}(\mathtt{R}) = \mathbf{\mathtt{trans}}_{\mathtt{e1}}(\mathbf{\mathtt{L}}(\mathtt{R_1})) \mathtt{+_R} \ldots \mathtt{+_R}~\mathbf{\mathtt{init}}(\mathtt{R})$ where  $\mathtt{R_1}, \ldots, \mathtt{R_k}$ are the neighbors of R $\mathtt{e_1}, \ldots, \mathtt{e_k}$ are the edges connecting those neighbors to ROnce we have a solution, we can examine its properties.  For instance, given a solution L, we could ask whether L(R) = None for any router R, indicating that that router receives no route from the destination and hence cannot reach it.  We could also ask what the length of the longest path from any node to the destination is.  By using the next-hop fields of routes, we can also reconstruct the path that any router uses to reach the destination.  If we were concerned with a property of that path, such as whether it traverses a particular waypoint (like $\mathtt{R_2}$ or $\mathtt{R_1}$) then we could deduce that as well. Summary: Routing is the process of computing paths to a given destination or a collection of destinations.  There are many different distributed routing protocols that engage in this process; they go by names such as BGP, OSPF, ISIS and others. It turns out these protocols have a lot of structure in common.  We can see that commonality by defining a class of models with the components (G, M, trans, merge, init):    G  —  A graph representing the topology of the network (with vertices V and edges E).     M  —  The set M of messages, also called routes, used by the protocol.     trans : E → M → M  —  The function that propagates, transforms or discards messages/routes as they travel across each edge in the network.     merge : V → M → M → M  —  The function that combines incoming information from neighboring routers, usually selecting one (or more) most preferred route for traffic traveling to the given destination.     init : V → M  —  The initial routes (init) at each node in the graph, which are used in the absence of receiving additional information about where or how to route traffic.  Given these components, we have enough information to simulate a model and find its solution, or stable state.  And once we have a solution in hand, we can analyze it to determine what sorts of properties it has.  For instance, we can determine whether a given router can reach a particular destination, or whether a computed route passes through a particular waypoint.  Such properties can help us uncover important bugs in network configurations before they are deployed. There’s a lot more to learn about this topic, and in future blog posts, we will explore some of them.  Can a system have more than one solution? How can we tell? Are there algorithms for finding them all? What happens when there are failures? Can we reason about quantitative properties like congestion? How do we construct models of real protocols like BGP, OSPF and their interactions? How does one actually implement network simulation and verification tools based on these models? References: [1] The stable paths problem and interdomain routing. T. G. Griffin, F. B. Shepherd, G. Wilfong.  IEEE/ACM Transactions on Networking 10(2).  April 2002. https://ieeexplore. ieee. org/document/993304 [2] An algebraic theory of dynamic routing.  J. L.  Sobrinho.  IEEE/ACM Transactions on Networking 13(5). Oct 2005. https://ieeexplore. ieee. org/document/1528502 [4] NV: Tools for modeling and analyzing network configurations.  June 2020. https://github. com/NetworkVerificationhttps://github. com/NetworkVerification/nv [5] NV: An Intermediate Language for Verification of Network Control Planes. Nick Giannarakis, Devon Loehr, Ryan Beckett and David Walker.  ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI). June 2020. https://www. cs. princeton. edu/~dpw/papers/nv-pldi20. pdf "
    }, {
    "id": 28,
    "url": "http://localhost:4000/its-the-equivalence-classes-stupid/",
    "title": "It’s the Equivalence Classes, Stupid",
    "body": "2020/05/26 - James Carville coined the phrase “It’s the economy, stupid” to say that what mattered in an election year was the economy. Formal methods search through vast state spaces in reasonable time by automatically finding (often implicit) equivalence classes of the state space, or by exploiting modularity or abstraction. But for some domains like networking, it appears better to precompute the equivalence classes before verification explicitly. Let me develop this idea in this post, targeted to networking folks not familiar with formal methods and to formal methods folks not familiar with networking. For networking readers, formal methods treat a system as a mathematical object. We then prove (i. e. , by covering all cases) that some property holds. For example: “source $S$ cannot send a packet to destination $D$ over any path” to enforce security.  While the classical way (Floyd-Hoare) was to write proofs or (recently) use proof assistants like Coq (which require human ingenuity), I will concentrate on algorithmic search techniques that are more “turnkey” for network operators. Algorithmic search methods search across the entire state space to verify a property. They include tools like SAT solvers, model checkers, and symbolic execution systems. All have found success in networks. And yet to scale to say a large data center network, these classical tools have to be modified to work, grouping the state space into sets. I start my network verification class by giving some idea of the state space for the $S$ to $D$ reachability problem. Assume that the only header fields that matter for forwarding are the destination and source IP address, and the TCP destination and source ports (used in Access Control Lists). Assuming IPv4, that gives $32 + 32 + 16 + 16 = 96$ bits. Thus the state space is more than $2^{96}$, which seems astronomical.  How in the world – I ask them – can we search through this large state space in reasonable time? Having puzzled them, I then ask them whether to figure out how packets go to UCLA from an ISP do we have to consider all possible UCLA IP addresses? The lights come on as they realize that it is possible to “compress” say all $\texttt{UCLA}/16$ ($128. 16. *$) into one equivalence class. As the packets come closer to UCLA, the equivalence classes may change to the departments (see Figure). Of course, it’s more complicated than simply writing down the department names because of ACLs and the fact that different routers may “see” different equivalence classes. But the basic intuition is sound. The first generation of network verification tools reduced network verification to classical verification tools. For example, Anteater from UIUC reduced its reachability to a SAT formula, and Flowchecker from UNC reduced its reachability to model checking. Intuitively, reachability can be expressed as a Boolean formula assuming finite paths because a packet can reach from $S$ to $D$ if for some path $P$ (the OR of all paths from $S$ to $D$) all the routers in $P$ forward the packet to the next router (AND across all routers). Classical tools also scale by working symbolically in terms of sets of states. Consider a SAT formula: $S = (a \land b) \lor (c \land d)$, where $a$, $b$, $c$, and $d$, are boolean variables that can be set to true or false. Is there some assignment of truth values to $a$, $b$, $c$, and $d$ such that S is true? The DPLL search procedure uses a search tree that branches on variable values. After branching on $a$ and $b$, the procedure can conclude that the formula is satisfiable $(a = \mathtt{True}, b = \mathtt{True})$ without considering $c$ or $d$. Thus a SAT solver works symbolically in terms of sets of states. Modern SAT solvers use CDCL-based algorithms with clause learning (which essentially learns implicit equivalences automatically). Similarly, model checkers today are symbolic – they work with sets of states. For example, when a model checker considers a program line $“\mathtt{If}\ x &lt; 9\ \mathtt{then}\ a = 5\ \mathtt{else}\ a = 10”$, it does not search through all possible values of $x$, but considers a tree that branches on the value of the Boolean formula $(x &lt; 9)$. Unfortunately, both Anteater (based on SAT) and Flowchecker (based on model checking) did not scale to large networks. I conjecture this is because the sets computed do not naturally align with the natural equivalence classes for networking (e. g. , $\mathtt{UCLA}. *$ that my students intuitively grasped). I became an accidental tourist in the world of formal methods when I did a sabbatical at Stanford visiting my friend Nick McKeown in 2004. Nick’s student Peyman Kazemian had invented a ternary algebra for reasoning about shared SDN networks. We persuaded Peyman to apply it to general IP networks (SDN wasn’t fashionable yet) to compute tasks like packet reachability and loops. We were clueless about existing work like Anteater and FlowChecker. The ternary algebra (where a bit could be either a $1$, $0$ or the wildcard character $*$) could easily model prefix and packet classification rules. But it could also model a set of packets moving through the network, in the same way, an equivalence class, as a wild card expression. For example, all packets destined to UCLA from an ISP would be a single wildcard expression. Note that equivalence classes are sets as in symbolic reasoning but are sets defined by an equivalence (symmetric, transitive, reflexive) relation, so there is a bit more structure. While this technique, which we called Header Space Analysis, was elegant, we found it was as slow as a tortoise. The header expressions were breaking into too many pieces as the tool hauled them through the network. In other words, the equivalence classes were fragmenting unnecessarily. Consider a router contained two rules matching the destination address $1*$ and $11*$ (analogous to $\mathtt{UCLA}. *$ and $\mathtt{UCLA. CS}. *$). The classes that matched $11*$ and not $1*$ (IP does longest matching prefix) were being split into $2^{31}$ pieces. So we tried representing wildcard expressions as subtractions (e. g. , $\mathtt{UCLA}. * - \texttt{UCLA}. \mathtt{CS}. *$). What made it work is distributivity. When a difference expression intersects with a rule at the router, intersection distributes across the difference. Classes begin as differences and stay as differences; the tool scaled and ran on the Stanford backbone in seconds. Later, when we described this to folks from formal methods, they looked embarrassed and told us that ternary algebras were well known in hardware verification, but had fallen out of favor compared to SAT solvers. While we were clearly naive, Peyman founded a company called Forward Networks, which uses his ternary algebra as a foundation. Perhaps this is because ternary algebra and difference compression are natural to networks with prefix and packet classification rules. We would not have found these domain-specific insights if we had used a model checker or SAT solver. Shortly after my sabbatical, I joined Microsoft Research and met Nikolaj Bjørner. Nikolaj conceived of modifying his Datalog engine in Z3 to get all solutions to network reachability and recruited a terrific intern called Nuno Lopes. However, Nuno quickly found he had to add a difference of cubes data structure (and a new relational operator where he combined Select with Project) to get the engine to scale to what we called NoD (or Network Optimized Datalog). NoD is more flexible than HSA because it can easily be extended. HSA and NoD scale using what one might call run-time dynamic equivalence classes. As packets move in the network, the equivalence classes are adjusted by adding a difference, removing null sets, etc. The Veriflow engine from UIUC used a clever idea that I call compile-time static equivalence classes. Before property checking, they used a trie-like structure to divide all possible network classes into equivalence classes. Unlike in HSA, these classes do not change at run-time. Veriflow was pioneering but perhaps a little ad hoc. The full fruition of this idea came with the Atomic Predicates paper by Yang and Lam. Consider a router with only $2$ forwarding entries for $\mathtt{UCLA}. *$ going to interface $I_1$ and $\mathtt{UCLA. CS}. *$ going to interface $I_2$ (see Figure). Header space analysis will compute all the differences in the wildcard expressions (e. g. , $*- \mathtt{UCLA}$, which will be dropped, and $\mathtt{UCLA}. *$ - $\mathtt{UCLA. CS}. *$, which goes to $I_1$. These new expressions have to be matched at every subsequent router and then checked whether they are null, both expensive operations.  Instead, Atomic Predicates notices that there are three equivalence classes of packets, packets that match neither rule ($*-\mathtt{UCLA}. *$), packets that match the first entry, and not the second ($\mathtt{UCLA}. *- \mathtt{UCLA. CS}. *$) and packets that match only the third entry ($\mathtt{UCLA. CS}. *$). Each equivalence class is given a number ($1$, $2$, and $3$). The router rules are rewritten as $2 → I_1$, and $3→ I_2$. Reachability is computed by injecting the set of all packets from the source as the set $\{1, 2, 3\}$. Class $1$ gets dropped, class $2$ is forwarded to $I_1$, and Class $3$ is forwarded to $I_2$. For a networking person, this is analogous to multi-protocol label switching (MPLS). We replace complex longest matching prefix and packet classification with integer labels that are set up before forwarding starts to make lookup faster. There are more details, but it comes closest to formally capture my students’ intuition as to why network verification should scale. The number of natural equivalence classes like $\mathtt{UCLA. CS}. *$ should be small. Another insight from networking explains why atomic predicates work well. An early paper by Lakshman and Staliadis on packet classification showed that packet classification (ACL lookup) at a single router is equivalent to finding all hypercubes that match a point in a 5-dimensional space. There is a lower bound in computational geometry that shows that this geometric problem requires either large space or time. However, routers like Cisco’s CRS-1 use an algorithm called HyperCuts that work well in practice. Most networking people believe this is so because the number of disjoint geometric regions induced by a classifier are small in practice though they could be large in theory.  The same reasoning explains why the number of atomic predicates even in a big network is quite small in practice. There are other twists on equivalence classes. For example, at MSR, we observed that complex data centers had lots of backup links and routers that were essentially similar. While this is classically interpreted as symmetries in the state space, one can view the MSR work as defining equivalence classes on the topology; by contrast, Atomic predicates define equivalence classes on the headers. Again this makes sense because the state for network reachability is a pair (packet, interface) where the packet is the current version of the header (which could be rewritten) and the interface is the interface in the network that the header packet is currently at while traveling from $S$ to $D$. While classical work in model checking looks for symmetries/equivalence class on the state space, in networks it makes sense to separately factor the equivalence of states into the equivalence of headers (Yang-Lam) and equivalence of interfaces. Later, Bonsai exploited symmetries for BGP routing, and Panda exploits symmetries in VMN to scale verification of stateful data planes like NAT. So are we done? Far from it. First, most progress has been on the data plane. Ryan Beckett and collaborators have advanced the control plane, but Minesweeper does not scale to large networks. Stateful forwarding is still hard despite Panda’s work. But the bigger problem that remains even for dataplane verification is the lack of specifications. I will write about that in a post called “Look Ma, no specs” about some work to address this problem at UCLA. There are other ways to reduce time to navigate large state spaces. We can use modularity as Todd Millstein points out where the state space is factored into smaller pieces, each with a smaller state space. Second, we can use abstraction: we transform the original network state space into a more abstract state space at the price of losing the ability to verify some properties. For example, Minesweeper abstracts away message passing of routing messages using solutions to the Stable Paths Problem. While modularity and abstraction are essential in other domains, perhaps precomputed equivalence classes are so successful in networks because networking is a shallower (not to mention finite) domain compared to programs or hardware. I once described networking verification to Ed Clarke. His main question was: what is the state? (the (header, packet) pair described above). While I learned this is an essential question, I believe an equally important question to learn why a verification tool scales is: what are the natural equivalence classes for the domain, and how does the verification algorithm find them? "
    }, {
    "id": 29,
    "url": "http://localhost:4000/you-cant-verify-what-you-cant-specify/",
    "title": "You can't verify what you can't specify",
    "body": "2020/05/11 - Back when I was studying for my master degree, Prof. Axel van Lamsweerde wasteaching us formal logic. Axel is world-famous for his works on requirementsengineering, that is, the process of defining and maintaining the prerequisitesthat must be met by software systems. During that time, I remember reading one of Axel’s seminal paper entitled“Formal Specification: a Roadmap” which describes thestrengths and weaknesses of formal specification technologies. I re-discovered Axel’s paper as we started to work on making network verification technologies more usable (more on this later) and found its lessons to be invaluable. In this post, I’d like to quickly summarize Axel’s views on why writing goodspecifications is challenging; why I believe these challenges apply almostverbatim to network verification; and what we can do to address them. I’ll also mention some of our recent work on automatically mining network specifications. The problem with formal specifications: As Axel puts it, a formal specification is “the expression, in some formallanguage and at some level of abstraction, of a collection of properties somesystem should satisfy”. Of course, not all formal specifications are useful. Axel goes therefore a bit further in defining “good” specifications as thosesatisfying a set of key properties: (i) they adequately capture the problem athand; (ii) they are consistent and unambiguous; (iii) they are complete; andyet, at the same time, (iv) they are minimal. Writing good specifications is difficult, perhaps as difficult as writing a correct program in the first place. Axel lists many reasons behind this complexity. Let me describe the ones that resonated with me the most. To start with, one needs to figure out what the properties to specify actuallyare. Specifications are indeed never formal in the first place: one must figurethem out. Doing so requires people with different background, mental models, andlanguages to come together (e. g. , customers, domain experts, architects, programmers). Finding a common ground is hard and often leads to inconsistencies. Once the properties of interest are known, they need to be expressed in somekind of formal language. Doing so is again hard as most users lack the relevantexpertise in formal languages (e. g. , mathematical logic). Specificationlanguages also provide little guidance to the user on how to elaborate a goodspecification (e. g. , constructive methods). Besides, they tend to focus on capturing functional properties (what the system is expectedto do), leaving out important non-functional properties. Despite being two decades old, I believe the problems Axel pointed at are still largely relevant. This recent tweet from Heidy Khlaaf (@HeidyKhlaaf) illustrates this particularly well: In the past three years of working on large safety critical systems, I&#39;ve learned that verification isn&#39;t the real problem, but it&#39;s writing specifications. Don&#39;t @ me. https://t. co/k4zi1j5QkX &mdash; Dr Heidy Khlaaf (هايدي خلاف) (@HeidyKhlaaf) December 15, 2019 Back to the network: At this point, you’re probably thinking “what does all this have to do with the network anyway?” Well, when it comes to network verification, pretty mucheverything. Both network verification and synthesis require a formalspecification of the intended network behavior, and writing thisspecification manually is difficult, if not downright impossible in some cases. As an exercise, put yourself in the shoes of a network operator working for,say, a large Internet Service Provider (ISP). Tired of human-induced downtimes,your boss asks you, from now on, to verify the network configurations beforepushing changes into production. Before using a verification tool though, youneed to specify what it is that you want the network to do… A few generic requirements come to mind: surely you want the network to “ensurereachability”, even if there are failures. Who wants forwarding loops or blackholes anyway? Those obvious requirements out of the way, you start to realizethat your network does way more than just ensuring reachability… Amongothers, it load-balances traffic, routes it so as to minimize transitcosts, isolates important customer traffic on disjoint links, and reroute partsof it via predefined waypoints. Thinking even further, non-functionalrequirements start to come to mind such as the need to converge rapidly uponfailures or to maintain the number of routes below a reasonable threshold. Figuring out the entire specification quickly becomes daunting, especially asmost of it has been homegrown over years by an entire team of network engineers(some of whom most likely do not even work there anymore). Writing the specification is daunting too. Since most (existing) networkspecification languages focus on capturing low-level forwarding and routingproperties, the specification ends up being gigantic. As an illustration, theformal specification for Internet2 (the US research network, a network composedof around 10 routers) contains up to thousands of predicates. Imagine writingthis by hand, without making any mistake. Besides, important non-functionalproperties (like fast convergence) cannot be specified and, therefore, cannotbe verified. Looking forward: So what do we do? Again, Axel’spaper describes manyinteresting avenues for future research… in networking. Like Axel, one of themain avenue I see pertains to designing higher-level specification languages, alittle bit like Frenetic did for OpenFlow. Ideally, theselanguages should be multiparadigm, to cope with the diversity of the users, and modular, to build large-scale specification from smallercomponents. Finally, these languages should also support non-functional properties such as convergence time, memory requirements, or maintainability. Another promising avenue, and one which we started to explore in a recentpaper, is to partially automate the task of writing the specifications. InConfig2Spec, we propose to do this by extracting thespecification from a network configuration. Once the full specification isextracted, the operator can then focus her attention on formalizing the deltas with respect to an existing network behavior. Further techniques are required besides languages and mining techniques though. In particular, like Axel, I believe network analysis tools should enablereasoning in spite of errors, based on partial and possibly bogusspecifications (e. g. , conflicting ones). Here, I could imagine systems in whichthe specification is incrementally refined and debunked with the help of ahuman. All this is probably for another post though… Stay tuned! Our group is recruiting!: If you are interested in network verification and synthesis, ourgroup has open PhD and post-doc positions in the area! Please reach out! "
    }, {
    "id": 30,
    "url": "http://localhost:4000/toward-modular-network-verification/",
    "title": "Toward modular network verification",
    "body": "2020/05/01 - Almost all of the techniques for network verification to date must analyze the entire network’s state and/or configuration data monolithically.  In contrast, methodologies for verification in other domains support modular (or compositional) reasoning: it is possible to verify a system by separately verifying its parts.  Modularity allows verification to scale to large systems, which is a key challenge for network verification today, and it provides many other system development and management benefits.  This post explains what modular verification is, why it is important, and how we might achieve forms of modular network verification. What is modular verification?: In a modular approach to verification, each component of a system is given a specification, and each component is separately verified to meet its specification.  Importantly, verifying each component only requires access to the specifications, not the implementations, of the components with which it interacts. As an example, consider a simple software system containing a main function $f$ that calls another function $g$.  To prove that $f$ meets some intended specification $\varphi$, a non-modular approach would analyze the behaviors of $f$ and $g$ jointly.  A modular approach, which is common in traditional software verification based on pre- and postconditions, is to instead define a specification $\varphi’$ for $g$ and thereby break the verification of $f$ into two independent parts:    Prove that $ g $ meets the specification $\varphi’$.     Prove that $f$ meets the specification $\varphi$, under the assumption that $g$ meets the specification $\varphi’$.  Why modular verification?: Modularity will provide a number of important benefits for network verification that are absent today. Scalability. Verification must explore all possible system behaviors.  Since the number of system behaviors can grow exponentially in the system size, monolithic verification is inherently unscalable.  And indeed, today the most complex forms of network verification, for example ensuring that a network’s control plane satisfies desired reachability properties in the face of any set of $k$ link failures, do not scale to large networks.  Modular verification can provide asymptotic gains in performance.  Roughly, if there are $n$ components each with $m$ possible behaviors, then the system as a whole can have up to $m^n$ behaviors, while a modular approach will only explore $m * n$ behaviors. Independent development.  Since each component’s correctness only depends on the specifications, but not the implementations, of the other components, modular verification allows each component to be developed and validated independently.  In the context of network verification, this means that different teams can be responsible for managing and validating different parts of a network, without requiring access to the configurations of other parts. Incremental re-verification.  Similarly, as the system is updated, it can be incrementally re-verified.  For example, if a component $\mathcal{C}$ changes but its specification is unchanged, then only $\mathcal{C}$ must be re-verified.  Network configurations are changed daily, to enable new services, perform security updates, etc.  Fast incremental re-verification is critical to making network verification practical. Design verification.  Finally, an under-appreciated benefit of modular verification is that it verifies not only the end-to-end properties of a system but also the system’s internal design.  A good system has a modular structure, with different components responsible for different tasks, and networks are no exception (see the next section).  Modular verification directly validates this structure by verifying that each component properly performs its tasks, and verification failures are localized to the relevant component and task within the system. Modular network verification: Networks have a lot of structure that can be exploited for modular verification, at different granularities. Subnetwork modularity. Many networks are composed of multiple subnetworks.  For example, a data center consists of multiple pods, and an enterprise network consists of multiple campuses. Protocol modularity. Networks often run multiple protocols, each of which has a different purpose, for example OSPF for intradomain routing and BGP for interdomain routing. Role modularity. Nodes in a network play different roles.  For example, a data center’s nodes are partitioned into leaf nodes, spine nodes, etc. , and an enterprise network’s nodes are partitioned into border routers, core routers, etc. Function modularity.  A router typically has multiple interfaces, each with its own policy.  Even for a single interface, the router configuration has separate components for different tasks, such as routing and access control. I am only aware of two existing verification techniques that exploit this modular structure, and both target data-plane verification in data centers.  First, Plotkin et al. perform modular reachability checks on certain subnetworks in order to speed up verification, a technique that they call “surgery. ” Second, Jayaraman et al. perform verification via separate, role-specific checks on each node in the network. Technical challenges: I believe that the field is ready to begin exploring all of the forms of modularity described above.  Some common technical challenges will arise, leading to important new insights and capabilities: What are the right specifications? A good specification must be abstract, so that it hides much of the complexity of a component from the rest of the system, while also being precise enough to allow desired system properties to be validated.  Research is necessary to identify appropriate component specifications for each of the above forms of modularity.  I believe that this research direction will also lead to a new understanding of best practices for network design and configuration, for example to minimize dependencies across sub-networks in routing configuration. Where do specifications come from? Initially I expect component specifications to be written manually by tool developers or users.  Longer term, as in other forms of verification the community should investigate forms of automated specification inference for network components.  This direction can likely build on recent work on network-wide specification inference and on role-level configuration template inference. How to ensure end-to-end properties? Depending on the form of modularity exploited, modular verification may not directly ensure properties of the entire network.  For example, even if each router properly plays its network role, that does not directly imply end-to-end reachability properties.  Initially I expect the connection between modular verification and network-wide properties to be either unproven or proven manually.  Longer term, the community should investigate ways to automatically connect per-component specifications to end-to-end network properties. Summary: Now that the community has laid many of the foundations for network verification, exploiting modularity is a natural next step.  I believe that modularity will be necessary to make network verification a practical technology for real networks.  I’m excited to pursue this vision and hope that others will join me in making it a reality. "
    }, {
    "id": 31,
    "url": "http://localhost:4000/2-current-state-of-research/",
    "title": "Capturing the state of research on network verification",
    "body": "2020/04/20 - Verification and synthesis are old problems in computer science. Verification seeks to answer the question: “can any input to a program result in that program producing an incorrect output. ” Likewise, synthesis seeks to answer the question: “can I generate a program that produces the correct output for every input. ” The motivation for verification and synthesis comes from the practical realization that humans often make mistakes and commonly introduce bugs into their systems. Having a way automatically (and quickly) find/fix all bugs in a system is highly appealing, particularly when such systems become sufficiently complex. While traditionally researchers have developed formal methods to verify computer hardware and software, recent trends in networking have exacerbated the need for such methods for networks as well. For example, the rise of cloud networking powered by the creation of enormous data center networks has dramatically raised the operational complexity of those networks. At the same time, as more services move online, it has become more vital that networks be bug-free and reliable. In response, researchers have started exploring the use of formal methods for networking. While they have made substantial progress on some problems, others remain unsolved. This article presents our view of the state of research on network verification. One category of work in this space aims to prove that network devices implement their functionality correctly (e. g. , NICE and Dobrescu et al. ). Another category aims to prove that the behavior of a given network is as intended by the operators. It typically assumes that the network devices are operating correctly; unintented network behavior can emerge even if all devices are bug-free when the network is not configured correctly. We will focus on the second category of work because the first one is closer to general software verification, and it is in the latter category where a lot of progress has been made recently. We will focus on first category of network verification and on network synthesis in future articles. The perspective in this article is shaped by our research and a course we taught that coalesced the ideas in this space. (We thank our collaborators and students for all we have learned from them. ) We realize that our perspective is inherently biased and incomplete and that we risk irking research colleagues who do not agree with our characterization (or whose work we failed to include). We invite those researchers to share their own perspectives. The first wave: Data plane verification: Some of the earliest work on network verification was adopted for network “data planes”. The data plane refers to the part of the network responsible for forwarding packets from point A to point B. In general, network forwarding is performed by a collection of “switches” that each maintain a forwarding table that matches a packet entering the switch and determines which port(s) the packet should go out of. The matching mechanism depends on the network. Traditional routers match packets based on the longest matching prefix, yet newer technologies such as OpenFlow and P4 allow for more general packet matching. In addition, routers along the path may even transform or encapsulate/decapsulate packets. Data plane verification aims to prove properties like node A cannot reach B, independent of the packet headers it chooses; node A can reach B; or no packet in network will loop. The challenge is to provide such guarantees for all possible packet headers (of which there are tens of billions). This challenge gets harder in the presence of packet transformations and encapsulations because those operations change the headers. Drawing on basic verification tools invented originally for other domains, networking researchers have developed a range of approaches to tractably reason about all possible packet headers. Some of the first data plane verification work encoded network forwarding tables and properties of interest as SAT constraints. The SAT solver’s declaration that constraints are unsatisfiable or the satisfying variable assignments indicated if the property held for the network. This approach was first outlined by Xie et al. and then shown to work on real networks in Anteater. Header Space Analysis used ternary simulation to efficiently propagate the set of all possible packets through the forwarding tables to find where every input packet will end up. This approach scaled better and it could identify all possible packets that violated a property, where the SAT-based approach identified only one counterexample. Later approaches to data plane verification improved upon Anteater and HSA in one dimension or another, such as  making the analysis incremental (e. g. , Veriflow, NetPlumber, Delta-net); making the analysis faster by identifying equivalence classes of packet (e. g. , Atomic Predicates; making the analysis faster identifying equivalences across parts of the network (e. g. , Symmetry and Surgery; decomposing (some) global properties into local properties (e. g. , RCDC). The second wave: Control plane verification: While data plane verification involves analyzing how packets are forwarded according to the tables present at every switch, in real networks these tables are themselves populated by other protocols or software, known as the “control plane” of the network.  The appeal of analyzing the control plane is that it can help prove the correctness of network behavior proactively, before that behavior emerges in the network. Network control planes typically come in two flavors: (1) distributed routing protocols, or (2) a centralized orchestrator. Since the centralized orchestrator is typically written in a general purpose programming language, traditional software verification techniques such as those based on Hoare Logic apply. However, automatically verifying arbitrary software is undecidable in general, and highly challenging in practice. As a result, arguably, researchers have had more success with (1). For example, Batfish simulates the protocols to their fixed points, which produces the forwarding tables, and it then leverages data plane verification tools to check for various kinds of bugs. Such simulations, however, can only explore one fixed point, and any environmental changes such as a link failure or an external route advertisement requires re-executing the simulation. This limitation makes it intractable to prove correctness for arbitrary changes. Minesweeper and BagPipe encode the fixed-point of distributed protocols as a function of the network environment using SMT. They can then use SMT solvers like Z3 to search for a network environment, such as a combination of failures, that will result in undesirable forwarding behavior. However, these tools are significantly less scalable than Batfish. As with data plane verification, later work improves control plane analysis along one dimension or another, such as  making the analysis faster using custom graph encodings for the fixed point (e. g. , ARC, Tiramisu); improving simulation speed for networks with monotonic control planes (e. g. , FastPlane; using explicit-state model checking to explore a range of environments (e. g. , Plankton) making the analysis faster by identifying equivalences across parts of the network (e. g. , Bonsai, Origami); making the analysis faster by using abstract interpretation (e. g. , ShapeShifter)The third wave: Stateful and programmable data planes: The first wave of data plane verification work largely tackled “stateless” dataplanes where how a packet was treated did not depend on prior packets. However, not all network data planes adhere to this model. A network could have middleboxes that perform stateful packet processing, such as denial-of-service protection logic that is based on all packets seen in the recent past. Additionally, an emerging class of devices allow for “programmable” dataplanes (e. g. , Barefoot Tofino), where network engineers can flexibly define how packets are processed. These capabilities of network devices enable exciting new functionalities and promise to allow networks to evolve more quickly. However, they also make verification more challenging since the data plane can now perform complex logic. Work that tackles such data planes is in its infancy, but as with stateless dataplane verification, researchers are exploring different verification approaches. Two of the early works, VMN and SymNet, employed symbolic execution and SMT encoding respectively for verifying dataplanes with middleboxes. More recently, NetSMC used model checking for this domain. An important effort in the domain of programmable dataplace is p4v, which leverages verification condition generation to reason about such programs with assumptions about the control plane. Fortunately, the absence of loops or recursion in data plane programs (since they must forward at line-rate) allows tools like p4v to be fully automatic. So what problems have been solved?: Perhaps the most clearly “solved” problem in network verification is that of stateless data plane verification, which also happens to be the earliest work in this space. Stateless data plane verification tools today can already scale to handle large networks with millions of forwarding table rules and thousands of routers, all at human time scales. Further work in this area has also revealed additional optimizations that make such tools even more scalable. These tools have been successful enough to find their way into practical use at large cloud providers such as Amazon and Microsoft and offered commercially by startups such as Forward Networks, Intentionet, and Veriflow. Control plane verification of individual network fixed-points is also a solved problem. Tools such as Batfish and FastPlane can quickly compute and verify the fixed point for large networks. This approach too is in production use at companies such as Microsoft and offered commercially by Intentionet. So what problems remain open?: Although verifying stateless data planes is largely a solved problem, the closely related problem of verifying stateful and programmable data planes is unsolved. The early work in the domain of middlebox proccessing is promising but many difficult problems related to scale and realism are not fully solved. Similarly, for programmable networks, while p4v can verify the forwarding functionality of a single switch, it is not possible to verify the correctness of an entire network of programmable switches. Given that many use cases for programmable switches require coordination, this is an important next step. Even further away is the possibility of jointly verifying both the control and data planes for networks with programmable devices. Such a joint analysis is promising because it could reduce the specification annotation burden on users (as required by p4v) since control plane invariants could be learned directly instead of inferred. Control plane verification for a range of fixed points, while possible today, remains challenging to scale to large networks. Recent works such as Plankton and Tiramisu have made reasoning about failures more scalable, but reasoning about route advertisements from other networks remains problematic. For the latter, Minesweeper and BagPipe are the only tools capable of such reasoning, but they do not scale beyond a few hundred devices. One could speculate if reasoning about arbitrary external route advertisements is worth the additional complication. For many networks, these routes may be known or highly constrained. For other networks, a practical approach may be to verify which routes a network allows in as a precondition. Summary: Network verification is a timely research area that holds the promise of increasing the reliability of critical network infrastructure. Researchers have made rapid strides over the last decade, with ideas already deployed in the world’s largest networks. Several problems remain, however, and we look forward to the research and engineering communities addressing them effectively over the next few years. "
    }, {
    "id": 32,
    "url": "http://localhost:4000/1-welcome-to-netverify/",
    "title": "Welcome to netverify.fun",
    "body": "2020/04/20 - Network verification and synthesis has emerged as an exciting research area at the intersection of networking, programming languages, and formal methods. Work in this area is motivated by the rapidly growing scale and complexity of modern networks and by the high frequency of network outages and security breaches. It aims to formally prove the correctness of (some aspects of) the network, raise the level of abstraction for network design, or develop methods that are correct by construction. Researchers have invented a range of approaches that cleverly combine ideas from hardware and software verification with domain-specific insights rooted in how networks function. The real-world impact of this line of research is undeniable despite the fact that the research is relatively new. All large cloud providers are now using some form of network verification to ensure high availability for their networks. And several startups have emerged to translate promising research to mature technologies that can be used by a broad array of network engineers. One side-effect of the newness of the area, and the speed at which it is evolving, is that it can be difficult to make sense of it, especially for researchers and practitioners who are not actively working in the area. It can be difficult to understand newly-developed techniques and difficult to tell how they fit into the overall picture (or, even, what is the overall picture?!). It is similarly difficult to tell which challenges have been effectively addressed and whichs ones remain open. These difficulties in turn make it harder for new researchers to enter into the area and for practitioners to judge which techniques are “shovel ready” for adoption in real networks. Our goal with netverify. fun is to make network verification and synthesis research accessible to a broad audience. We intend for it to be a living site that explains the latest research, contains expert commentary that synthesizes what may appear to be disjointed developments, and provides opinions on important open problems. The site of course cannot succeed without community engagement. If you work in this area, we invite you to write articles that explain your work to (non-networking) computer scientists and network engineers or, more generally, provide your perspective on the field. If you are a network engineer, we invite you to share your challenges and views on how existing research falls short. If you are a researcher that does not work in this area, we invite you to express your frustrations with research in the area or tell us what you’d like to see covered. Our intent is for the site to be diverse and inclusive of all viewpoints. Contributing to the site is easy. Send pull requests to its GitHub repository with new articles or additions/corrections to existing ones; open issues for topics you would like to see covered; and participate in discussions about published articles. You are also welcome to drop us a line if you need a sounding board for your contribution ideas. We are launching netverify. fun site with a piece that captures our view on network verification research, and we have a great line up of articles in the pipeline. So, stay tuned! "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Search for keywords..."> 
</form>

            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
<div class="row justify-content-center">
    <div class="col-md-8">        
        <div class="row align-items-center mb-5">
            <div class="col-md-9">
                <h2 class="font-weight-bold">Hongqiang Harry Liu <span class="small btn btn-outline-success btn-sm btn-round"><a href="https://ratul.org/">View</a></span></h2>
                <p class="excerpt">Hongqiang Harry Liu is a Director of Research in Alibaba Cloud and Alibaba DAMO Acedamy. He holds a PhD from the Department of Computer Science at Yale University and a bachelor's degree and a master's degree from the Department of Electronic Engineering at Tsinghua University. Before joining Alibaba, he served as a researcher at Microsoft Research Asia and was responsible for the R&D and implementation of key technologies related to stable, high-performance Azure networks. His research interests cover cloud data center networks, backbone networks, mobile network transmission technologies, 5G networks, and device-side computing. He has served as a reviewer of papers in SIGCOMM and NSDI conferences and has published nearly 20 papers accepted by SIGCOMM, NSDI, and SOSP conferences. In 2014, he won the SIGCOMM Doctoral Dissertation Award - Honorable Mention.</p>
            </div>
            <div class="col-md-3 text-right">
                <img alt="Hongqiang Harry Liu" src="http://localhost:4000//assets/images/harry-liu.png" class="rounded-circle" height="100" width="100">
            </div>
        </div>
        <h4 class="font-weight-bold spanborder"><span>Posts by Hongqiang Harry Liu</span></h4>
        
        
        <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/the-practice-of-network-verification-in-alibaba-global-wan/">The Practice of Network Verification in Alibaba’s Global WAN</a>
	</h2>
	<p class="excerpt">
	   Alibaba has a global scale infrastructure to support its various types of online services (e.g. e-commerce, cloud computing, e-payment, etc.), which have more than one billion users i...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#research">research</a><span class="sep">, </span>
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#industry experience">industry experience</a><span class="sep">, </span>
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#network verification">network verification</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="d-block text-muted">
		By <span class="catlist">
		
		
		<a class="text-capitalize text-muted smoothscroll" href="http://www.hongqiangliu.com/">Hongqiang Harry Liu</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		May 08, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/the-practice-of-network-verification-in-alibaba-global-wan/">
	<img class="w-100" src="/assets/images/hoyan.png" alt="The Practice of Network Verification in Alibaba’s Global WAN">
	</a>
	</div>

</div>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    </div>
</div>
</div>

    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>


    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Netverify</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>
            </div>
	    <div>
                <!--  Github Repo Star Btn-->
                <a class="text-dark ml-1" target="_blank" href="https://github.com/netverify/netverify.github.io"><i class="fab fa-github"></i> Submit an article via github</a>
	    </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


    <script>
        MathJax = {
          tex : {
            tags : 'ams',
            inlineMath : [ ['$','$'] ],
            processEscapes : false,
            processEnvironments : false,
            processRefs : false,
          }
        }
    </script>
    <script src='https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-chtml.js'
              integrity='sha256-h37zcxt3siZItdbF3C1YCiJz+FuG45k8SERUkNDAvko=' crossorigin>
    </script>

</body>

</html>
